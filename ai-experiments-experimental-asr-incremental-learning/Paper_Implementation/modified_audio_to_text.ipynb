{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"modified_audio_to_text.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMZP5R/fXr0KuPKPD30tXXP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install nemo_toolkit['all']\n","!pip install hydra-core==1.1"],"metadata":{"id":"OX4yu8dbs57x"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkKcawiJpmvP"},"outputs":[],"source":["import io\n","import math\n","import os\n","from typing import Callable, Dict, Iterable, List, Optional, Union\n","\n","import braceexpand\n","import numpy as np\n","import torch\n","import webdataset as wd\n","from torch.utils.data import ChainDataset\n","\n","from nemo.collections.asr.parts.preprocessing.features import WaveformFeaturizer\n","from nemo.collections.common import tokenizers\n","from nemo.collections.common.parts.preprocessing import collections, parsers\n","from nemo.core.classes import Dataset, IterableDataset\n","from nemo.core.neural_types import *\n","from nemo.utils import logging\n","\n","import pickle"]},{"cell_type":"code","source":["__all__ = [\n","    'AudioToBPEDataset',\n","    'TarredAudioToCharDataset',\n","    'TarredAudioToBPEDataset',\n","]"],"metadata":{"id":"iCNXoz1fs9NZ","executionInfo":{"status":"ok","timestamp":1656148657323,"user_tz":-330,"elapsed":10,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Overridden collate_fn of PyTorch Dataset"],"metadata":{"id":"Z6yYev1evF8q"}},{"cell_type":"code","source":["def _speech_collate_fn(batch, pad_id):\n","    \"\"\"collate batch of audio sig, audio len, tokens, tokens len, teacher model \n","    output logits, teacher model self attention layer output maps\n","    Args:\n","        batch (Optional[FloatTensor], Optional[LongTensor], LongTensor,\n","               LongTensor, FloatTensor, FloatTensor):  A tuple of tuples of \n","               signal, signal lengths, encoded tokens, encoded tokens length, \n","               teacher model logits and teacher model self attention layer \n","               outputs.  This collate func assumes the signals are 1d torch \n","               tensors (i.e. mono audio), logits and maps are 2d torch tensors.\n","    \"\"\"\n","    packed_batch = list(zip(*batch))\n","    if len(packed_batch) == 7:\n","        _, audio_lengths, _, tokens_lengths, logits, maps, sample_ids = packed_batch\n","    elif len(packed_batch) == 6:\n","        sample_ids = None\n","        _, audio_lengths, _, tokens_lengths, logits, maps = packed_batch\n","    else:\n","        raise ValueError(\"Expects 6 or 7 tensors in the batch!\")\n","    max_audio_len = 0\n","    has_audio = audio_lengths[0] is not None\n","    if has_audio:\n","        max_audio_len = max(audio_lengths).item()\n","    max_tokens_len = max(tokens_lengths).item()\n","\n","    audio_signal, tokens = [], []\n","    for b in batch:\n","        if len(b) == 7:\n","            sig, sig_len, tokens_i, tokens_i_len, logits, maps, _ = b\n","        else:\n","            sig, sig_len, tokens_i, tokens_i_len, logits, maps = b\n","        if has_audio:\n","            sig_len = sig_len.item()\n","            if sig_len < max_audio_len:\n","                pad = (0, max_audio_len - sig_len)\n","                sig = torch.nn.functional.pad(sig, pad)\n","            audio_signal.append(sig)\n","        tokens_i_len = tokens_i_len.item()\n","        if tokens_i_len < max_tokens_len:\n","            pad = (0, max_tokens_len - tokens_i_len)\n","            tokens_i = torch.nn.functional.pad(tokens_i, pad, value=pad_id)\n","        tokens.append(tokens_i)\n","\n","    if has_audio:\n","        audio_signal = torch.stack(audio_signal)\n","        audio_lengths = torch.stack(audio_lengths)\n","    else:\n","        audio_signal, audio_lengths = None, None\n","    tokens = torch.stack(tokens)\n","    tokens_lengths = torch.stack(tokens_lengths)\n","\n","    if sample_ids is None:\n","        return audio_signal, audio_lengths, tokens, tokens_lengths, logits, maps\n","    else:\n","        sample_ids = torch.tensor(sample_ids, dtype=torch.int32)\n","        return audio_signal, audio_lengths, tokens, tokens_lengths, logits, maps, sample_ids"],"metadata":{"id":"By79BDnJtw_0","executionInfo":{"status":"ok","timestamp":1656148657324,"user_tz":-330,"elapsed":10,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class ASRManifestProcessor:\n","    \"\"\"\n","    Class that processes a manifest json file containing paths to audio files, transcripts, and durations (in seconds).\n","    Each new line is a different sample. Example below:\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text_filepath\": \"/path/to/audio.txt\", \"duration\": 23.147}\n","    ...\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"the transcription\", \"offset\": 301.75, \"duration\": 0.82, \"utt\":\n","    \"utterance_id\", \"ctm_utt\": \"en_4156\", \"side\": \"A\"}\n","    Args:\n","        manifest_filepath: Path to manifest json as described above. Can be comma-separated paths.\n","        parser: Str for a language specific preprocessor or a callable.\n","        max_duration: If audio exceeds this length, do not include in dataset.\n","        min_duration: If audio is less than this length, do not include in dataset.\n","        max_utts: Limit number of utterances.\n","        bos_id: Id of beginning of sequence symbol to append if not None.\n","        eos_id: Id of end of sequence symbol to append if not None.\n","        pad_id: Id of pad symbol. Defaults to 0.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        manifest_filepath: str,\n","        parser: Union[str, Callable],\n","        max_duration: Optional[float] = None,\n","        min_duration: Optional[float] = None,\n","        max_utts: int = 0,\n","        bos_id: Optional[int] = None,\n","        eos_id: Optional[int] = None,\n","        pad_id: int = 0,\n","        index_by_file_id: bool = False,\n","    ):\n","        self.parser = parser\n","\n","        self.collection = collections.ASRAudioText(\n","            manifests_files=manifest_filepath,\n","            parser=parser,\n","            min_duration=min_duration,\n","            max_duration=max_duration,\n","            max_number=max_utts,\n","            index_by_file_id=index_by_file_id,\n","        )\n","\n","        self.eos_id = eos_id\n","        self.bos_id = bos_id\n","        self.pad_id = pad_id\n","\n","    def process_text_by_id(self, index: int) -> (List[int], int):\n","        sample = self.collection[index]\n","        return self.process_text_by_sample(sample)\n","\n","    def process_text_by_file_id(self, file_id: str) -> (List[int], int):\n","        manifest_idx = self.collection.mapping[file_id][0]\n","        sample = self.collection[manifest_idx]\n","        return self.process_text_by_sample(sample)\n","\n","    def process_text_by_sample(self, sample: collections.ASRAudioText.OUTPUT_TYPE) -> (List[int], int):\n","        t, tl = sample.text_tokens, len(sample.text_tokens)\n","\n","        if self.bos_id is not None:\n","            t = [self.bos_id] + t\n","            tl += 1\n","        if self.eos_id is not None:\n","            t = t + [self.eos_id]\n","            tl += 1\n","\n","        return t, tl"],"metadata":{"id":"IjHcsFpSxiY8","executionInfo":{"status":"ok","timestamp":1656148657324,"user_tz":-330,"elapsed":9,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def expand_audio_filepaths(audio_tar_filepaths, shard_strategy: str, world_size: int, global_rank: int):\n","    valid_shard_strategies = ['scatter', 'replicate']\n","    if shard_strategy not in valid_shard_strategies:\n","        raise ValueError(f\"`shard_strategy` must be one of {valid_shard_strategies}\")\n","\n","    if isinstance(audio_tar_filepaths, str):\n","        # Replace '(' and '[' with '{'\n","        brace_keys_open = ['(', '[', '<', '_OP_']\n","        for bkey in brace_keys_open:\n","            if bkey in audio_tar_filepaths:\n","                audio_tar_filepaths = audio_tar_filepaths.replace(bkey, \"{\")\n","\n","        # Replace ')' and ']' with '}'\n","        brace_keys_close = [')', ']', '>', '_CL_']\n","        for bkey in brace_keys_close:\n","            if bkey in audio_tar_filepaths:\n","                audio_tar_filepaths = audio_tar_filepaths.replace(bkey, \"}\")\n","\n","    if isinstance(audio_tar_filepaths, str):\n","        # Brace expand\n","        audio_tar_filepaths = list(braceexpand.braceexpand(audio_tar_filepaths))\n","\n","    # Check for distributed and partition shards accordingly\n","    if world_size > 1:\n","        if shard_strategy == 'scatter':\n","            logging.info(\"All tarred dataset shards will be scattered evenly across all nodes.\")\n","\n","            if len(audio_tar_filepaths) % world_size != 0:\n","                logging.warning(\n","                    f\"Number of shards in tarred dataset ({len(audio_tar_filepaths)}) is not divisible \"\n","                    f\"by number of distributed workers ({world_size}).\"\n","                )\n","\n","            begin_idx = (len(audio_tar_filepaths) // world_size) * global_rank\n","            end_idx = begin_idx + len(audio_tar_filepaths) // world_size\n","            audio_tar_filepaths = audio_tar_filepaths[begin_idx:end_idx]\n","            logging.info(\n","                \"Partitioning tarred dataset: process (%d) taking shards [%d, %d)\", global_rank, begin_idx, end_idx\n","            )\n","\n","        elif shard_strategy == 'replicate':\n","            logging.info(\"All tarred dataset shards will be replicated across all nodes.\")\n","        else:\n","            raise ValueError(f\"Invalid shard strategy ! Allowed values are : {valid_shard_strategies}\")\n","\n","    return audio_tar_filepaths"],"metadata":{"id":"EjfRPl7T2its","executionInfo":{"status":"ok","timestamp":1656148658215,"user_tz":-330,"elapsed":899,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class _AudioTextDataset(Dataset):\n","    \"\"\"\n","    Dataset that loads tensors via a json file containing paths to audio files, transcripts, and durations (in seconds).\n","    Each new line is a different sample. Example below:\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text_filepath\": \"/path/to/audio.txt\", \"duration\": 23.147}\n","    ...\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"the transcription\", \"offset\": 301.75, \"duration\": 0.82, \"utt\":\n","    \"utterance_id\", \"ctm_utt\": \"en_4156\", \"side\": \"A\"}\n","    Args:\n","        manifest_filepath: Path to manifest json as described above. Can be comma-separated paths.\n","        parser: Str for a language specific preprocessor or a callable.\n","        sample_rate (int): Sample rate to resample loaded audio to\n","        int_values (bool): If true, load samples as 32-bit integers. Defauts to False.\n","        augmentor (nemo.collections.asr.parts.perturb.AudioAugmentor): An AudioAugmentor object used to augment loaded\n","            audio\n","        max_duration: If audio exceeds this length, do not include in dataset\n","        min_duration: If audio is less than this length, do not include in dataset\n","        max_utts: Limit number of utterances\n","        trim: whether or not to trim silence. Defaults to False\n","        bos_id: Id of beginning of sequence symbol to append if not None\n","        eos_id: Id of end of sequence symbol to append if not None\n","        pad_id: Id of pad symbol. Defaults to 0\n","        return_sample_id (bool): whether to return the sample_id as a part of each sample\n","    \"\"\"\n","\n","    @property\n","    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n","        \"\"\"Returns definitions of module output ports. (collate_fn returns this)\n","               \"\"\"\n","        return {\n","            'audio_signal': NeuralType(('B', 'T'), AudioSignal()),\n","            'a_sig_length': NeuralType(tuple('B'), LengthsType()),\n","            'transcripts': NeuralType(('B', 'T'), LabelsType()),\n","            'transcript_length': NeuralType(tuple('B'), LengthsType()),\n","            'teacher_logits': NeuralType(('B', 'T', 'D'), LogprobsType()),\n","            'teacher_feature_map': NeuralType(('B', 'T', 'D'), LogprobsType()),\n","            'sample_id': NeuralType(tuple('B'), LengthsType(), optional=True),\n","        }\n","\n","    def __init__(\n","        self,\n","        manifest_filepath: str,\n","        parser: Union[str, Callable],\n","        sample_rate: int,\n","        int_values: bool = False,\n","        augmentor: 'nemo.collections.asr.parts.perturb.AudioAugmentor' = None,\n","        max_duration: Optional[int] = None,\n","        min_duration: Optional[int] = None,\n","        max_utts: int = 0,\n","        trim: bool = False,\n","        bos_id: Optional[int] = None,\n","        eos_id: Optional[int] = None,\n","        pad_id: int = 0,\n","        return_sample_id: bool = False,\n","    ):\n","        if type(manifest_filepath) == str:\n","            manifest_filepath = manifest_filepath.split(\",\")\n","\n","        self.manifest_processor = ASRManifestProcessor(\n","            manifest_filepath=manifest_filepath,\n","            parser=parser,\n","            max_duration=max_duration,\n","            min_duration=min_duration,\n","            max_utts=max_utts,\n","            bos_id=bos_id,\n","            eos_id=eos_id,\n","            pad_id=pad_id,\n","        )\n","        self.featurizer = WaveformFeaturizer(sample_rate=sample_rate, int_values=int_values, augmentor=augmentor)\n","        self.trim = trim\n","        self.return_sample_id = return_sample_id\n","\n","    def get_manifest_sample(self, sample_id):\n","        return self.manifest_processor.collection[sample_id]\n","\n","    def __getitem__(self, index):\n","        # A method that specifies how to load a single data instance\n","\n","        # Loading objects (data) from files (on Disk) to restore their values to memory\n","        # Loading teacher_logits\n","        file = \"teacher_logits.pkl\"\n","        file_obj = open(file, \"rb\") \n","        teacher_logits = pickle.load(file_obj) # Tensor\n","\n","        # Loading teacher_feature_map\n","        file = \"teacher_feature_map.pkl\"\n","        file_obj = open(file, \"rb\") # read binary\n","        teacher_feature_map = pickle.load(file_obj) # Tensor\n","\n","        sample = self.manifest_processor.collection[index]\n","        offset = sample.offset\n","\n","        if offset is None:\n","            offset = 0\n","\n","        features = self.featurizer.process(\n","            sample.audio_file, offset=offset, duration=sample.duration, trim=self.trim, orig_sr=sample.orig_sr\n","        )\n","        f, fl = features, torch.tensor(features.shape[0]).long() # Tensor, Tensor\n","\n","        t, tl = self.manifest_processor.process_text_by_sample(sample=sample) # List, Int\n","\n","        if self.return_sample_id:\n","            output = f, fl, torch.tensor(t).long(), torch.tensor(tl).long(), teacher_logits[index], teacher_feature_map[index], index\n","        else:\n","            output = f, fl, torch.tensor(t).long(), torch.tensor(tl).long(), teacher_logits[index], teacher_feature_map[index]\n","\n","        return output\n","\n","    def __len__(self):\n","        # A method that returns the total number of data instances\n","        return len(self.manifest_processor.collection)\n","\n","    def _collate_fn(self, batch):\n","        return _speech_collate_fn(batch, pad_id=self.manifest_processor.pad_id)"],"metadata":{"id":"rDjjuQHh3MmQ","executionInfo":{"status":"ok","timestamp":1656148658216,"user_tz":-330,"elapsed":9,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class AudioToBPEDataset(_AudioTextDataset):\n","    \"\"\"\n","    Dataset that loads tensors via a json file containing paths to audio\n","    files, transcripts, and durations (in seconds). Each new line is a\n","    different sample. Example below:\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text_filepath\":\n","    \"/path/to/audio.txt\", \"duration\": 23.147}\n","    ...\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"the\n","    transcription\", \"offset\": 301.75, \"duration\": 0.82, \"utt\":\n","    \"utterance_id\", \"ctm_utt\": \"en_4156\", \"side\": \"A\"}\n","\n","    In practice, the dataset and manifest used for character encoding and byte pair encoding\n","    are exactly the same. The only difference lies in how the dataset tokenizes the text in\n","    the manifest.\n","\n","    Args:\n","        manifest_filepath: Path to manifest json as described above. Can\n","            be comma-separated paths.\n","        tokenizer: A subclass of the Tokenizer wrapper found in the common collection,\n","            nemo.collections.common.tokenizers.TokenizerSpec. ASR Models support a subset of\n","            all available tokenizers.\n","        sample_rate (int): Sample rate to resample loaded audio to\n","        int_values (bool): If true, load samples as 32-bit integers. Defauts to False.\n","        augmentor (nemo.collections.asr.parts.perturb.AudioAugmentor): An AudioAugmentor\n","            object used to augment loaded audio\n","        max_duration: If audio exceeds this length, do not include in dataset\n","        min_duration: If audio is less than this length, do not include\n","            in dataset\n","        max_utts: Limit number of utterances\n","        trim: Whether to trim silence segments\n","        use_start_end_token: Boolean which dictates whether to add [BOS] and [EOS]\n","            tokens to beginning and ending of speech respectively.\n","        return_sample_id (bool): whether to return the sample_id as a part of each sample\n","    \"\"\"\n","\n","    @property\n","    def output_types(self) -> Optional[Dict[str, NeuralType]]:\n","        \"\"\"Returns definitions of module output ports.\n","               \"\"\"\n","        return {\n","            'audio_signal': NeuralType(('B', 'T'), AudioSignal()),\n","            'a_sig_length': NeuralType(tuple('B'), LengthsType()),\n","            'transcripts': NeuralType(('B', 'T'), LabelsType()),\n","            'transcript_length': NeuralType(tuple('B'), LengthsType()),\n","            'teacher_logits': NeuralType(('B', 'T', 'D'), LogprobsType()),\n","            'teacher_feature_map': NeuralType(('B', 'T', 'D'), LogprobsType()),\n","            'sample_id': NeuralType(tuple('B'), LengthsType(), optional=True),\n","        }\n","\n","    def __init__(\n","        self,\n","        manifest_filepath: str,\n","        tokenizer: 'nemo.collections.common.tokenizers.TokenizerSpec',\n","        sample_rate: int,\n","        int_values: bool = False,\n","        augmentor: 'nemo.collections.asr.parts.perturb.AudioAugmentor' = None,\n","        max_duration: Optional[int] = None,\n","        min_duration: Optional[int] = None,\n","        max_utts: int = 0,\n","        trim: bool = False,\n","        use_start_end_token: bool = True,\n","        return_sample_id: bool = False,\n","    ):\n","        if use_start_end_token and hasattr(tokenizer, 'bos_token'):\n","            bos_id = tokenizer.bos_id\n","        else:\n","            bos_id = None\n","\n","        if use_start_end_token and hasattr(tokenizer, 'eos_token'):\n","            eos_id = tokenizer.eos_id\n","        else:\n","            eos_id = None\n","\n","        if hasattr(tokenizer, 'pad_token'):\n","            pad_id = tokenizer.pad_id\n","        else:\n","            pad_id = 0\n","\n","        class TokenizerWrapper:\n","            def __init__(self, tokenizer):\n","                if isinstance(tokenizer, tokenizers.aggregate_tokenizer.AggregateTokenizer):\n","                    self.is_aggregate = True\n","                else:\n","                    self.is_aggregate = False\n","                self._tokenizer = tokenizer\n","\n","            def __call__(self, *args):\n","                t = self._tokenizer.text_to_ids(*args)\n","                return t\n","\n","        super().__init__(\n","            manifest_filepath=manifest_filepath,\n","            parser=TokenizerWrapper(tokenizer),\n","            sample_rate=sample_rate,\n","            int_values=int_values,\n","            augmentor=augmentor,\n","            max_duration=max_duration,\n","            min_duration=min_duration,\n","            max_utts=max_utts,\n","            bos_id=bos_id,\n","            eos_id=eos_id,\n","            pad_id=pad_id,\n","            trim=trim,\n","            return_sample_id=return_sample_id,\n","        )"],"metadata":{"id":"X9MrlMBiCR3R","executionInfo":{"status":"ok","timestamp":1656148658216,"user_tz":-330,"elapsed":8,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class _TarredAudioToTextDataset(IterableDataset):\n","    \"\"\"\n","    A similar Dataset to the AudioToCharDataset/AudioToBPEDataset, but which loads tarred audio files.\n","\n","    Accepts a single comma-separated JSON manifest file (in the same style as for the AudioToCharDataset/AudioToBPEDataset),\n","    as well as the path(s) to the tarball(s) containing the wav files. Each line of the manifest should\n","    contain the information for one audio file, including at least the transcript and name of the audio\n","    file within the tarball.\n","\n","    Valid formats for the audio_tar_filepaths argument include:\n","    (1) a single string that can be brace-expanded, e.g. 'path/to/audio.tar' or 'path/to/audio_{1..100}.tar.gz', or\n","    (2) a list of file paths that will not be brace-expanded, e.g. ['audio_1.tar', 'audio_2.tar', ...].\n","\n","    Note: For brace expansion in (1), there may be cases where `{x..y}` syntax cannot be used due to shell interference.\n","    This occurs most commonly inside SLURM scripts. Therefore we provide a few equivalent replacements.\n","    Supported opening braces - { <=> (, [, < and the special tag _OP_.\n","    Supported closing braces - } <=> ), ], > and the special tag _CL_.\n","    For SLURM based tasks, we suggest the use of the special tags for ease of use.\n","\n","    See the WebDataset documentation for more information about accepted data and input formats.\n","\n","    If using multiple workers the number of shards should be divisible by world_size to ensure an\n","    even split among workers. If it is not divisible, logging will give a warning but training will proceed.\n","    In addition, if using mutiprocessing, each shard MUST HAVE THE SAME NUMBER OF ENTRIES after filtering\n","    is applied. We currently do not check for this, but your program may hang if the shards are uneven!\n","\n","    Notice that a few arguments are different from the AudioToCharDataset; for example, shuffle (bool) has been\n","    replaced by shuffle_n (int).\n","\n","    Additionally, please note that the len() of this DataLayer is assumed to be the length of the manifest\n","    after filtering. An incorrect manifest length may lead to some DataLoader issues down the line.\n","\n","    Args:\n","        audio_tar_filepaths: Either a list of audio tarball filepaths, or a\n","            string (can be brace-expandable).\n","        manifest_filepath (str): Path to the manifest.\n","        parser (callable): A callable which is used to pre-process the text output.\n","        sample_rate (int): Sample rate to resample loaded audio to\n","        int_values (bool): If true, load samples as 32-bit integers. Defauts to False.\n","        augmentor (nemo.collections.asr.parts.perturb.AudioAugmentor): An AudioAugmentor\n","            object used to augment loaded audio\n","        shuffle_n (int): How many samples to look ahead and load to be shuffled.\n","            See WebDataset documentation for more details.\n","            Defaults to 0.\n","        min_duration (float): Dataset parameter.\n","            All training files which have a duration less than min_duration\n","            are dropped. Note: Duration is read from the manifest JSON.\n","            Defaults to 0.1.\n","        max_duration (float): Dataset parameter.\n","            All training files which have a duration more than max_duration\n","            are dropped. Note: Duration is read from the manifest JSON.\n","            Defaults to None.\n","        max_utts (int): Limit number of utterances. 0 means no maximum.\n","        blank_index (int): Blank character index, defaults to -1.\n","        unk_index (int): Unknown character index, defaults to -1.\n","        normalize (bool): Dataset parameter.\n","            Whether to use automatic text cleaning.\n","            It is highly recommended to manually clean text for best results.\n","            Defaults to True.\n","        trim (bool): Whether to use trim silence from beginning and end\n","            of audio signal using librosa.effects.trim().\n","            Defaults to False.\n","        bos_id (id): Dataset parameter.\n","            Beginning of string symbol id used for seq2seq models.\n","            Defaults to None.\n","        eos_id (id): Dataset parameter.\n","            End of string symbol id used for seq2seq models.\n","            Defaults to None.\n","        pad_id (id): Token used to pad when collating samples in batches.\n","            If this is None, pads using 0s.\n","            Defaults to None.\n","        shard_strategy (str): Tarred dataset shard distribution strategy chosen as a str value during ddp.\n","            -   `scatter`: The default shard strategy applied by WebDataset, where each node gets\n","                a unique set of shards, which are permanently pre-allocated and never changed at runtime.\n","            -   `replicate`: Optional shard strategy, where each node gets all of the set of shards\n","                available in the tarred dataset, which are permanently pre-allocated and never changed at runtime.\n","                The benefit of replication is that it allows each node to sample data points from the entire\n","                dataset independently of other nodes, and reduces dependence on value of `shuffle_n`.\n","\n","                Note: Replicated strategy allows every node to sample the entire set of available tarfiles,\n","                and therefore more than one node may sample the same tarfile, and even sample the same\n","                data points! As such, there is no assured guarantee that all samples in the dataset will be\n","                sampled at least once during 1 epoch.\n","        global_rank (int): Worker rank, used for partitioning shards. Defaults to 0.\n","        world_size (int): Total number of processes, used for partitioning shards. Defaults to 0.\n","        return_sample_id (bool): whether to return the sample_id as a part of each sample\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        audio_tar_filepaths: Union[str, List[str]],\n","        manifest_filepath: str,\n","        parser: Callable,\n","        sample_rate: int,\n","        int_values: bool = False,\n","        augmentor: Optional['nemo.collections.asr.parts.perturb.AudioAugmentor'] = None,\n","        shuffle_n: int = 0,\n","        min_duration: Optional[float] = None,\n","        max_duration: Optional[float] = None,\n","        max_utts: int = 0,\n","        trim: bool = False,\n","        bos_id: Optional[int] = None,\n","        eos_id: Optional[int] = None,\n","        pad_id: int = 0,\n","        shard_strategy: str = \"scatter\",\n","        global_rank: int = 0,\n","        world_size: int = 0,\n","        return_sample_id: bool = False,\n","    ):\n","        self.manifest_processor = ASRManifestProcessor(\n","            manifest_filepath=manifest_filepath,\n","            parser=parser,\n","            max_duration=max_duration,\n","            min_duration=min_duration,\n","            max_utts=max_utts,\n","            bos_id=bos_id,\n","            eos_id=eos_id,\n","            pad_id=pad_id,\n","            index_by_file_id=True,  # Must set this so the manifest lines can be indexed by file ID\n","        )\n","\n","        self.featurizer = WaveformFeaturizer(sample_rate=sample_rate, int_values=int_values, augmentor=augmentor)\n","        self.trim = trim\n","        self.eos_id = eos_id\n","        self.bos_id = bos_id\n","        self.pad_id = pad_id\n","        self.return_sample_id = return_sample_id\n","\n","        audio_tar_filepaths = expand_audio_filepaths(\n","            audio_tar_filepaths=audio_tar_filepaths,\n","            shard_strategy=shard_strategy,\n","            world_size=world_size,\n","            global_rank=global_rank,\n","        )\n","\n","        # Put together WebDataset\n","        self._dataset = wd.WebDataset(urls=audio_tar_filepaths, nodesplitter=None)\n","\n","        if shuffle_n > 0:\n","            self._dataset = self._dataset.shuffle(shuffle_n)\n","        else:\n","            logging.info(\"WebDataset will not shuffle files within the tar files.\")\n","\n","        self._dataset = (\n","            self._dataset.rename(audio='wav;ogg;flac', key='__key__')\n","            .to_tuple('audio', 'key')\n","            .pipe(self._filter)\n","            .pipe(self._loop_offsets)\n","            .map(f=self._build_sample)\n","        )\n","\n","    def _filter(self, iterator):\n","        \"\"\"This function is used to remove samples that have been filtered out by ASRAudioText already.\n","        Otherwise, we would get a KeyError as _build_sample attempts to find the manifest entry for a sample\n","        that was filtered out (e.g. for duration).\n","        Note that if using multi-GPU training, filtering may lead to an imbalance in samples in each shard,\n","        which may make your code hang as one process will finish before the other.\n","        \"\"\"\n","\n","        class TarredAudioFilter:\n","            def __init__(self, collection):\n","                self.iterator = iterator\n","                self.collection = collection\n","\n","            def __iter__(self):\n","                return self\n","\n","            def __next__(self):\n","                while True:\n","                    audio_bytes, audio_filename = next(self.iterator)\n","                    file_id, _ = os.path.splitext(os.path.basename(audio_filename))\n","                    if file_id in self.collection.mapping:\n","                        return audio_bytes, audio_filename\n","\n","        return TarredAudioFilter(self.manifest_processor.collection)\n","\n","    def _loop_offsets(self, iterator):\n","        \"\"\"This function is used to iterate through utterances with different offsets for each file.\n","        \"\"\"\n","\n","        class TarredAudioLoopOffsets:\n","            def __init__(self, collection):\n","                self.iterator = iterator\n","                self.collection = collection\n","                self.current_fn = None\n","                self.current_bytes = None\n","                self.offset_id = 0\n","\n","            def __iter__(self):\n","                return self\n","\n","            def __next__(self):\n","                if self.current_fn is None:\n","                    self.current_bytes, self.current_fn = next(self.iterator)\n","                    self.offset_id = 0\n","                else:\n","                    offset_list = self.collection.mapping[self.current_fn]\n","                    if len(offset_list) == self.offset_id + 1:\n","                        self.current_bytes, self.current_fn = next(self.iterator)\n","                        self.offset_id = 0\n","                    else:\n","                        self.offset_id += 1\n","\n","                return self.current_bytes, self.current_fn, self.offset_id\n","\n","        return TarredAudioLoopOffsets(self.manifest_processor.collection)\n","\n","    def _collate_fn(self, batch):\n","        return _speech_collate_fn(batch, self.pad_id)\n","\n","    def _build_sample(self, tup):\n","        \"\"\"Builds the training sample by combining the data from the WebDataset with the manifest info.\n","        \"\"\"\n","\n","        # Loading objects (data) from files (on Disk) to restore their values to memory\n","        # Loading teacher_logits\n","        file = \"teacher_logits.pkl\"\n","        file_obj = open(file, \"rb\") \n","        teacher_logits = pickle.load(file_obj) # Tensor\n","\n","        # Loading teacher_feature_map\n","        file = \"teacher_feature_map.pkl\"\n","        file_obj = open(file, \"rb\") # read binary\n","        teacher_feature_map = pickle.load(file_obj) # Tensor\n","\n","        audio_bytes, audio_filename, offset_id = tup\n","\n","        # Grab manifest entry from self.manifest_preprocessor.collection\n","        file_id, _ = os.path.splitext(os.path.basename(audio_filename))\n","        manifest_idx = self.manifest_processor.collection.mapping[file_id][offset_id]\n","        manifest_entry = self.manifest_processor.collection[manifest_idx]\n","\n","        offset = manifest_entry.offset\n","        if offset is None:\n","            offset = 0\n","\n","        # Convert audio bytes to IO stream for processing (for SoundFile to read)\n","        audio_filestream = io.BytesIO(audio_bytes)\n","        features = self.featurizer.process(\n","            audio_filestream,\n","            offset=offset,\n","            duration=manifest_entry.duration,\n","            trim=self.trim,\n","            orig_sr=manifest_entry.orig_sr,\n","        )\n","        audio_filestream.close()\n","\n","        # Audio features\n","        f, fl = features, torch.tensor(features.shape[0]).long()\n","\n","        # Text features\n","        t, tl = manifest_entry.text_tokens, len(manifest_entry.text_tokens)\n","\n","        self.manifest_processor.process_text_by_sample(sample=manifest_entry)\n","\n","        if self.bos_id is not None:\n","            t = [self.bos_id] + t\n","            tl += 1\n","        if self.eos_id is not None:\n","            t = t + [self.eos_id]\n","            tl += 1\n","\n","        if self.return_sample_id:\n","            return f, fl, torch.tensor(t).long(), torch.tensor(tl).long(), teacher_logits[manifest_idx], teacher_feature_map[manifest_idx], manifest_idx\n","        else:\n","            return f, fl, torch.tensor(t).long(), torch.tensor(tl).long(), teacher_logits[manifest_idx], teacher_feature_map[manifest_idx]\n","\n","    def get_manifest_sample(self, sample_id):\n","        return self.manifest_processor.collection[sample_id]\n","\n","    def __iter__(self):\n","        return self._dataset.__iter__()\n","\n","    def __len__(self):\n","        return len(self.manifest_processor.collection)"],"metadata":{"id":"lj1IN9aAEnOu","executionInfo":{"status":"ok","timestamp":1656148658638,"user_tz":-330,"elapsed":430,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class TarredAudioToCharDataset(_TarredAudioToTextDataset):\n","    \"\"\"\n","    A similar Dataset to the AudioToCharDataset, but which loads tarred audio files.\n","\n","    Accepts a single comma-separated JSON manifest file (in the same style as for the AudioToCharDataset),\n","    as well as the path(s) to the tarball(s) containing the wav files. Each line of the manifest should\n","    contain the information for one audio file, including at least the transcript and name of the audio\n","    file within the tarball.\n","\n","    Valid formats for the audio_tar_filepaths argument include:\n","    (1) a single string that can be brace-expanded, e.g. 'path/to/audio.tar' or 'path/to/audio_{1..100}.tar.gz', or\n","    (2) a list of file paths that will not be brace-expanded, e.g. ['audio_1.tar', 'audio_2.tar', ...].\n","\n","    See the WebDataset documentation for more information about accepted data and input formats.\n","\n","    If using multiple workers the number of shards should be divisible by world_size to ensure an\n","    even split among workers. If it is not divisible, logging will give a warning but training will proceed.\n","    In addition, if using mutiprocessing, each shard MUST HAVE THE SAME NUMBER OF ENTRIES after filtering\n","    is applied. We currently do not check for this, but your program may hang if the shards are uneven!\n","\n","    Notice that a few arguments are different from the AudioToCharDataset; for example, shuffle (bool) has been\n","    replaced by shuffle_n (int).\n","\n","    Additionally, please note that the len() of this DataLayer is assumed to be the length of the manifest\n","    after filtering. An incorrect manifest length may lead to some DataLoader issues down the line.\n","\n","    Args:\n","        audio_tar_filepaths: Either a list of audio tarball filepaths, or a\n","            string (can be brace-expandable).\n","        manifest_filepath (str): Path to the manifest.\n","        labels (list): List of characters that can be output by the ASR model.\n","            For Jasper, this is the 28 character set {a-z '}. The CTC blank\n","            symbol is automatically added later for models using ctc.\n","        sample_rate (int): Sample rate to resample loaded audio to\n","        int_values (bool): If true, load samples as 32-bit integers. Defauts to False.\n","        augmentor (nemo.collections.asr.parts.perturb.AudioAugmentor): An AudioAugmentor\n","            object used to augment loaded audio\n","        shuffle_n (int): How many samples to look ahead and load to be shuffled.\n","            See WebDataset documentation for more details.\n","            Defaults to 0.\n","        min_duration (float): Dataset parameter.\n","            All training files which have a duration less than min_duration\n","            are dropped. Note: Duration is read from the manifest JSON.\n","            Defaults to 0.1.\n","        max_duration (float): Dataset parameter.\n","            All training files which have a duration more than max_duration\n","            are dropped. Note: Duration is read from the manifest JSON.\n","            Defaults to None.\n","        max_utts (int): Limit number of utterances. 0 means no maximum.\n","        blank_index (int): Blank character index, defaults to -1.\n","        unk_index (int): Unknown character index, defaults to -1.\n","        normalize (bool): Dataset parameter.\n","            Whether to use automatic text cleaning.\n","            It is highly recommended to manually clean text for best results.\n","            Defaults to True.\n","        trim (bool): Whether to use trim silence from beginning and end\n","            of audio signal using librosa.effects.trim().\n","            Defaults to False.\n","        bos_id (id): Dataset parameter.\n","            Beginning of string symbol id used for seq2seq models.\n","            Defaults to None.\n","        eos_id (id): Dataset parameter.\n","            End of string symbol id used for seq2seq models.\n","            Defaults to None.\n","        pad_id (id): Token used to pad when collating samples in batches.\n","            If this is None, pads using 0s.\n","            Defaults to None.\n","        shard_strategy (str): Tarred dataset shard distribution strategy chosen as a str value during ddp.\n","            -   `scatter`: The default shard strategy applied by WebDataset, where each node gets\n","                a unique set of shards, which are permanently pre-allocated and never changed at runtime.\n","            -   `replicate`: Optional shard strategy, where each node gets all of the set of shards\n","                available in the tarred dataset, which are permanently pre-allocated and never changed at runtime.\n","                The benefit of replication is that it allows each node to sample data points from the entire\n","                dataset independently of other nodes, and reduces dependence on value of `shuffle_n`.\n","\n","                Note: Replicated strategy allows every node to sample the entire set of available tarfiles,\n","                and therefore more than one node may sample the same tarfile, and even sample the same\n","                data points! As such, there is no assured guarantee that all samples in the dataset will be\n","                sampled at least once during 1 epoch.\n","        global_rank (int): Worker rank, used for partitioning shards. Defaults to 0.\n","        world_size (int): Total number of processes, used for partitioning shards. Defaults to 0.\n","        return_sample_id (bool): whether to return the sample_id as a part of each sample\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        audio_tar_filepaths: Union[str, List[str]],\n","        manifest_filepath: str,\n","        labels: List[str],\n","        sample_rate: int,\n","        int_values: bool = False,\n","        augmentor: Optional['nemo.collections.asr.parts.perturb.AudioAugmentor'] = None,\n","        shuffle_n: int = 0,\n","        min_duration: Optional[float] = None,\n","        max_duration: Optional[float] = None,\n","        max_utts: int = 0,\n","        blank_index: int = -1,\n","        unk_index: int = -1,\n","        normalize: bool = True,\n","        trim: bool = False,\n","        bos_id: Optional[int] = None,\n","        eos_id: Optional[int] = None,\n","        parser: Optional[str] = 'en',\n","        pad_id: int = 0,\n","        shard_strategy: str = \"scatter\",\n","        global_rank: int = 0,\n","        world_size: int = 0,\n","        return_sample_id: bool = False,\n","    ):\n","        self.labels = labels\n","\n","        parser = parsers.make_parser(\n","            labels=labels, name=parser, unk_id=unk_index, blank_id=blank_index, do_normalize=normalize\n","        )\n","\n","        super().__init__(\n","            audio_tar_filepaths=audio_tar_filepaths,\n","            manifest_filepath=manifest_filepath,\n","            parser=parser,\n","            sample_rate=sample_rate,\n","            int_values=int_values,\n","            augmentor=augmentor,\n","            shuffle_n=shuffle_n,\n","            min_duration=min_duration,\n","            max_duration=max_duration,\n","            max_utts=max_utts,\n","            trim=trim,\n","            bos_id=bos_id,\n","            eos_id=eos_id,\n","            pad_id=pad_id,\n","            shard_strategy=shard_strategy,\n","            global_rank=global_rank,\n","            world_size=world_size,\n","            return_sample_id=return_sample_id,\n","        )"],"metadata":{"id":"r0VbsjP8GFgH","executionInfo":{"status":"ok","timestamp":1656148658639,"user_tz":-330,"elapsed":7,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class TarredAudioToBPEDataset(_TarredAudioToTextDataset):\n","    \"\"\"\n","    A similar Dataset to the AudioToBPEDataset, but which loads tarred audio files.\n","\n","    Accepts a single comma-separated JSON manifest file (in the same style as for the AudioToBPEDataset),\n","    as well as the path(s) to the tarball(s) containing the wav files. Each line of the manifest should\n","    contain the information for one audio file, including at least the transcript and name of the audio\n","    file within the tarball.\n","\n","    Valid formats for the audio_tar_filepaths argument include:\n","    (1) a single string that can be brace-expanded, e.g. 'path/to/audio.tar' or 'path/to/audio_{1..100}.tar.gz', or\n","    (2) a list of file paths that will not be brace-expanded, e.g. ['audio_1.tar', 'audio_2.tar', ...].\n","\n","    See the WebDataset documentation for more information about accepted data and input formats.\n","\n","    If using multiple workers the number of shards should be divisible by world_size to ensure an\n","    even split among workers. If it is not divisible, logging will give a warning but training will proceed.\n","    In addition, if using mutiprocessing, each shard MUST HAVE THE SAME NUMBER OF ENTRIES after filtering\n","    is applied. We currently do not check for this, but your program may hang if the shards are uneven!\n","\n","    Notice that a few arguments are different from the AudioToBPEDataset; for example, shuffle (bool) has been\n","    replaced by shuffle_n (int).\n","\n","    Additionally, please note that the len() of this DataLayer is assumed to be the length of the manifest\n","    after filtering. An incorrect manifest length may lead to some DataLoader issues down the line.\n","\n","    Args:\n","        audio_tar_filepaths: Either a list of audio tarball filepaths, or a\n","            string (can be brace-expandable).\n","        manifest_filepath (str): Path to the manifest.\n","        tokenizer (TokenizerSpec): Either a Word Piece Encoding tokenizer (BERT),\n","            or a Sentence Piece Encoding tokenizer (BPE). The CTC blank\n","            symbol is automatically added later for models using ctc.\n","        sample_rate (int): Sample rate to resample loaded audio to\n","        int_values (bool): If true, load samples as 32-bit integers. Defauts to False.\n","        augmentor (nemo.collections.asr.parts.perturb.AudioAugmentor): An AudioAugmentor\n","            object used to augment loaded audio\n","        shuffle_n (int): How many samples to look ahead and load to be shuffled.\n","            See WebDataset documentation for more details.\n","            Defaults to 0.\n","        min_duration (float): Dataset parameter.\n","            All training files which have a duration less than min_duration\n","            are dropped. Note: Duration is read from the manifest JSON.\n","            Defaults to 0.1.\n","        max_duration (float): Dataset parameter.\n","            All training files which have a duration more than max_duration\n","            are dropped. Note: Duration is read from the manifest JSON.\n","            Defaults to None.\n","        max_utts (int): Limit number of utterances. 0 means no maximum.\n","        trim (bool): Whether to use trim silence from beginning and end\n","            of audio signal using librosa.effects.trim().\n","            Defaults to False.\n","        use_start_end_token: Boolean which dictates whether to add [BOS] and [EOS]\n","            tokens to beginning and ending of speech respectively.\n","        pad_id (id): Token used to pad when collating samples in batches.\n","            If this is None, pads using 0s.\n","            Defaults to None.\n","        shard_strategy (str): Tarred dataset shard distribution strategy chosen as a str value during ddp.\n","            -   `scatter`: The default shard strategy applied by WebDataset, where each node gets\n","                a unique set of shards, which are permanently pre-allocated and never changed at runtime.\n","            -   `replicate`: Optional shard strategy, where each node gets all of the set of shards\n","                available in the tarred dataset, which are permanently pre-allocated and never changed at runtime.\n","                The benefit of replication is that it allows each node to sample data points from the entire\n","                dataset independently of other nodes, and reduces dependence on value of `shuffle_n`.\n","\n","                Note: Replicated strategy allows every node to sample the entire set of available tarfiles,\n","                and therefore more than one node may sample the same tarfile, and even sample the same\n","                data points! As such, there is no assured guarantee that all samples in the dataset will be\n","                sampled at least once during 1 epoch.\n","        global_rank (int): Worker rank, used for partitioning shards. Defaults to 0.\n","        world_size (int): Total number of processes, used for partitioning shards. Defaults to 0.\n","        return_sample_id (bool): whether to return the sample_id as a part of each sample\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        audio_tar_filepaths: Union[str, List[str]],\n","        manifest_filepath: str,\n","        tokenizer: 'nemo.collections.common.tokenizers.TokenizerSpec',\n","        sample_rate: int,\n","        int_values: bool = False,\n","        augmentor: Optional['nemo.collections.asr.parts.perturb.AudioAugmentor'] = None,\n","        shuffle_n: int = 0,\n","        min_duration: Optional[float] = None,\n","        max_duration: Optional[float] = None,\n","        max_utts: int = 0,\n","        trim: bool = False,\n","        use_start_end_token: bool = True,\n","        shard_strategy: str = \"scatter\",\n","        global_rank: int = 0,\n","        world_size: int = 0,\n","        return_sample_id: bool = False,\n","    ):\n","        if use_start_end_token and hasattr(tokenizer, 'bos_token'):\n","            bos_id = tokenizer.bos_id\n","        else:\n","            bos_id = None\n","\n","        if use_start_end_token and hasattr(tokenizer, 'eos_token'):\n","            eos_id = tokenizer.eos_id\n","        else:\n","            eos_id = None\n","\n","        if hasattr(tokenizer, 'pad_token'):\n","            pad_id = tokenizer.pad_id\n","        else:\n","            pad_id = 0\n","\n","        class TokenizerWrapper:\n","            def __init__(self, tokenizer):\n","                if isinstance(tokenizer, tokenizers.aggregate_tokenizer.AggregateTokenizer):\n","                    self.is_aggregate = True\n","                else:\n","                    self.is_aggregate = False\n","                self._tokenizer = tokenizer\n","\n","            def __call__(self, *args):\n","                t = self._tokenizer.text_to_ids(*args)\n","                return t\n","\n","        super().__init__(\n","            audio_tar_filepaths=audio_tar_filepaths,\n","            manifest_filepath=manifest_filepath,\n","            parser=TokenizerWrapper(tokenizer),\n","            sample_rate=sample_rate,\n","            int_values=int_values,\n","            augmentor=augmentor,\n","            shuffle_n=shuffle_n,\n","            min_duration=min_duration,\n","            max_duration=max_duration,\n","            max_utts=max_utts,\n","            trim=trim,\n","            bos_id=bos_id,\n","            eos_id=eos_id,\n","            pad_id=pad_id,\n","            shard_strategy=shard_strategy,\n","            global_rank=global_rank,\n","            world_size=world_size,\n","            return_sample_id=return_sample_id,\n","        )"],"metadata":{"id":"4-3dMFVTG6VL","executionInfo":{"status":"ok","timestamp":1656148658641,"user_tz":-330,"elapsed":8,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class BucketingDataset(IterableDataset):\n","    \"\"\"\n","    A Dataset which wraps another IterableDataset and adopts it for bucketing\n","    Args:\n","        dataset (IterableDataset): The IterableDataset to get wrapped\n","        bucketing_batch_size (int): Number of samples to build a batch\n","    \"\"\"\n","\n","    def __init__(\n","        self, dataset: IterableDataset, bucketing_batch_size: int,\n","    ):\n","        self.wrapped_dataset = dataset\n","        self.bucketing_batch_size = bucketing_batch_size\n","        super().__init__()\n","\n","    def _collate_fn(self, batch):\n","        return _speech_collate_fn(batch[0], self.wrapped_dataset.pad_id)\n","\n","    def __iter__(self):\n","        return BucketingIterator(\n","            wrapped_iter=self.wrapped_dataset._dataset.__iter__(), bucketing_batch_size=self.bucketing_batch_size\n","        ).__iter__()\n","\n","    def __len__(self):\n","        return int(math.ceil(len(self.wrapped_dataset) / float(self.bucketing_batch_size)))\n","\n","\n","class BucketingIterator:\n","    def __init__(self, wrapped_iter, bucketing_batch_size):\n","        self.wrapped_iter = wrapped_iter\n","        self.bucketing_batch_size = bucketing_batch_size\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        batches = []\n","        for idx in range(self.bucketing_batch_size):\n","            try:\n","                sample = next(self.wrapped_iter)\n","            except StopIteration:\n","                break\n","            batches.append(sample)\n","        if len(batches) == 0:\n","            raise StopIteration\n","        return batches\n","\n","\n","class RandomizedChainDataset(ChainDataset):\n","    def __init__(self, datasets: Iterable[Dataset], rnd_seed=0) -> None:\n","        super(RandomizedChainDataset, self).__init__(list(datasets))\n","        self.rnd_gen = np.random.RandomState(rnd_seed)\n","\n","    def __iter__(self):\n","        shuffled_order = self.rnd_gen.permutation(len(self.datasets))\n","        for dataset_idx in shuffled_order:\n","            d = self.datasets[dataset_idx]\n","            assert isinstance(d, IterableDataset), \"ChainDataset only supports IterableDataset\"\n","            for x in d:\n","                yield x"],"metadata":{"id":"JBgJdrNvHDE5","executionInfo":{"status":"ok","timestamp":1656148658641,"user_tz":-330,"elapsed":7,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":12,"outputs":[]}]}