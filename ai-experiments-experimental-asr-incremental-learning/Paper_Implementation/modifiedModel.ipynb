{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"modifiedModel.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMEXzjMKRhhT05V+DxmPZqV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7652e719f43143dca9c61ecdde26b715":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a7e306c54104937afcacf882ac38394","IPY_MODEL_e49d8d7bde7b4f20b897f6e6f53324f0","IPY_MODEL_d2e8321e6b8c4b5d9106e33449ed0b89"],"layout":"IPY_MODEL_ac0ffba13d5d4d6f9d1bdcc9f3aca73e"}},"8a7e306c54104937afcacf882ac38394":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42caaa978ab44a9791b8a691e3342f09","placeholder":"​","style":"IPY_MODEL_dbdf438a63d94fd99078b51f3ffcc6c3","value":"Transcribing: 100%"}},"e49d8d7bde7b4f20b897f6e6f53324f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ec2b969903241d3999081c9a6271268","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ea54643cc0d4e65b3fba35a7f3aca02","value":1}},"d2e8321e6b8c4b5d9106e33449ed0b89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_580ec694d65048afbed5b3b64bfb4cf0","placeholder":"​","style":"IPY_MODEL_129c57e92d2b4653b894906f9654df85","value":" 1/1 [00:02&lt;00:00,  2.27s/it]"}},"ac0ffba13d5d4d6f9d1bdcc9f3aca73e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42caaa978ab44a9791b8a691e3342f09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dbdf438a63d94fd99078b51f3ffcc6c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ec2b969903241d3999081c9a6271268":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ea54643cc0d4e65b3fba35a7f3aca02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"580ec694d65048afbed5b3b64bfb4cf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"129c57e92d2b4653b894906f9654df85":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["Installing Packages"],"metadata":{"id":"jhPolgHO6cUs"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"id":"w2qVYKq09Wes"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/Colab Notebooks/Paper 1 Implementation\""],"metadata":{"id":"tC4OJJp_BfO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3LB4EobS6HlZ"},"outputs":[],"source":["!pip install nemo_toolkit['all']\n","!pip install hydra-core==1.1"]},{"cell_type":"code","source":["!pip install import-ipynb"],"metadata":{"id":"JM2FSZVOFixW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Importing Libraries"],"metadata":{"id":"X3-aei2W6iZp"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import os\n","import tempfile\n","import json\n","import import_ipynb\n","import torchvision\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import pickle\n","import pandas as pd"],"metadata":{"id":"oo8KrGpo6Q5g","executionInfo":{"status":"ok","timestamp":1656158133216,"user_tz":-330,"elapsed":5796,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"J5P26M1XAyrs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from omegaconf import DictConfig, OmegaConf, open_dict\n","from typing import Dict, List, Optional, Union\n","from tqdm.auto import tqdm\n","from nemo.core.config import hydra_runner\n","from nemo.collections.asr.data.audio_to_text_dali import DALIOutputs\n","from nemo.collections.asr.metrics.wer import WER\n","from nemo.collections.asr.models.ctc_bpe_models import EncDecCTCModelBPE\n","from nemo.collections.asr.parts.mixins import ASRBPEMixin\n","from nemo.collections.asr.parts.mixins import ASRModuleMixin\n","from nemo.collections.asr.parts.preprocessing.perturb import process_augmentations\n","from nemo.core.classes.common import PretrainedModelInfo, typecheck\n","from nemo.core.neural_types import AudioSignal, LabelsType, LengthsType, LogprobsType, NeuralType, SpectrogramType\n","from nemo.utils import logging, model_utils\n","from nemo.utils import logging\n","%run modifiedDataset.ipynb\n","%run overwrittenEncoder.ipynb import OverwrittenEncoder\n","%run totalLoss.ipynb import TotalLoss"],"metadata":{"id":"_EgerLBBAx7o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["__all__ = ['ModifiedModel']"],"metadata":{"id":"vFu18RDU6XrR","executionInfo":{"status":"ok","timestamp":1656158353725,"user_tz":-330,"elapsed":556,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Modified Model Class with Overridden Methods"],"metadata":{"id":"XbCLt1476oCl"}},{"cell_type":"code","source":["class ModifiedModel(EncDecCTCModelBPE, ASRBPEMixin):\n","  def __init__(self, cfg: DictConfig, trainer=None):\n","    super().__init__(cfg=cfg, trainer=trainer)\n","\n","    self.encoder = OverwrittenEncoder(feat_in=80, feat_out=-1, n_layers=18, d_model=512, subsampling='striding', \n","      subsampling_factor=4, subsampling_conv_channels=-1, ff_expansion_factor=4, self_attention_model='rel_pos', n_heads=8, att_context_size=[-1, -1],\n","      xscaling=True, untie_biases=True, pos_emb_max_len=5000, conv_kernel_size=31, dropout=0.1, dropout_emb=0.0, dropout_att=0.1)\n","    \n","    self.loss = TotalLoss(num_classes=self.decoder.num_classes_with_blank - 1, reduction=self._cfg.get(\"ctc_reduction\", \"mean_batch\"),\n","        # hyper parameters\n","        temperature=3, gamma=500, beta=0.03)\n","\n","  def _setup_dataloader_from_config(self, config: Optional[Dict]):\n","    if 'augmentor' in config:\n","        augmentor = process_augmentations(config['augmentor'])\n","    else:\n","        augmentor = None\n","\n","    shuffle = config['shuffle']\n","    device = 'gpu' if torch.cuda.is_available() else 'cpu'\n","    if config.get('use_dali', False):\n","        device_id = self.local_rank if device == 'gpu' else None\n","        dataset = modifiedDataset.get_dali_bpe_dataset(\n","            config=config,\n","            tokenizer=self.tokenizer,\n","            shuffle=shuffle,\n","            device_id=device_id,\n","            global_rank=self.global_rank,\n","            world_size=self.world_size,\n","            preprocessor_cfg=self._cfg.preprocessor,\n","        )\n","        return dataset\n","\n","    # Instantiate tarred dataset loader or normal dataset loader\n","    if config.get('is_tarred', False):\n","        if ('tarred_audio_filepaths' in config and config['tarred_audio_filepaths'] is None) or (\n","            'manifest_filepath' in config and config['manifest_filepath'] is None\n","        ):\n","            logging.warning(\n","                \"Could not load dataset as `manifest_filepath` was None or \"\n","                f\"`tarred_audio_filepaths` is None. Provided config : {config}\"\n","            )\n","            return None\n","\n","        shuffle_n = config.get('shuffle_n', 4 * config['batch_size']) if shuffle else 0\n","        dataset = modifiedDataset.get_tarred_dataset(\n","            config=config,\n","            tokenizer=self.tokenizer,\n","            shuffle_n=shuffle_n,\n","            global_rank=self.global_rank,\n","            world_size=self.world_size,\n","            augmentor=augmentor,\n","        )\n","        shuffle = False\n","    else:\n","        if 'manifest_filepath' in config and config['manifest_filepath'] is None:\n","            logging.warning(f\"Could not load dataset as `manifest_filepath` was None. Provided config : {config}\")\n","            return None\n","\n","        dataset = modifiedDataset.get_bpe_dataset(\n","            config=config, tokenizer=self.tokenizer, augmentor=augmentor\n","        )\n","    if hasattr(dataset, 'collate_fn'):\n","        collate_fn = dataset.collate_fn\n","    else:\n","        collate_fn = dataset.datasets[0].collate_fn\n","\n","    return torch.utils.data.DataLoader(\n","        dataset=dataset,\n","        batch_size=config['batch_size'],\n","        collate_fn=collate_fn,\n","        drop_last=config.get('drop_last', False),\n","        shuffle=shuffle,\n","        num_workers=config.get('num_workers', 0),\n","        pin_memory=config.get('pin_memory', False),\n","    )\n","\n","  @torch.no_grad()\n","  def transcribe(\n","      self,\n","      paths2audio_files: List[str],\n","      batch_size: int = 4,\n","      logprobs: bool = False,\n","      return_hypotheses: bool = False,\n","      return_self_attention_outputs: bool = False,\n","      num_workers: int = 0,\n","  ) -> List[str]:\n","      \"\"\"\n","      Uses greedy decoding to transcribe audio files. Use this method for debugging and prototyping.\n","\n","      Args:\n","          paths2audio_files: (a list) of paths to audio files. \\\n","              Recommended length per file is between 5 and 25 seconds. \\\n","              But it is possible to pass a few hours long file if enough GPU memory is available.\n","          batch_size: (int) batch size to use during inference.\n","              Bigger will result in better throughput performance but would use more memory.\n","          logprobs: (bool) pass True to get log probabilities instead of transcripts.\n","          return_hypotheses: (bool) Either return hypotheses or text\n","              With hypotheses can do some postprocessing like getting timestamp or rescoring\n","          return_self_attention_outputs: (bool) pass True to get outputs of self attention layer instead of transcripts or log probabilities.\n","          num_workers: (int) number of workers for DataLoader\n","\n","      Returns:\n","          A list of transcriptions (or raw log probabilities (tensor) if logprobs is True, \n","          or outputs of self attention layer (tensor) if return_self_attention_outputs \n","          is True) in the same order as paths2audio_files\n","      \"\"\"\n","      if paths2audio_files is None or len(paths2audio_files) == 0:\n","          return {}\n","\n","      if (return_hypotheses and logprobs and return_self_attention_outputs) or (return_hypotheses and logprobs) or \\\n","      (logprobs and return_self_attention_outputs) or \\\n","      (return_hypotheses and return_self_attention_outputs):\n","          raise ValueError(\n","              \"Either `return_hypotheses` or `logprobs` or `return_self_attention_outputs` can be True at any given time.\"\n","              \"Returned hypotheses will contain the logprobs or the self attention layer outputs.\"\n","          )\n","\n","      if num_workers is None:\n","          num_workers = min(batch_size, os.cpu_count() - 1)\n","\n","      # We will store transcriptions here\n","      hypotheses = []\n","      # Model's mode and device\n","      mode = self.training\n","      device = next(self.parameters()).device\n","      dither_value = self.preprocessor.featurizer.dither\n","      pad_to_value = self.preprocessor.featurizer.pad_to\n","\n","      try:\n","          self.preprocessor.featurizer.dither = 0.0\n","          self.preprocessor.featurizer.pad_to = 0\n","          # Switch model to evaluation mode\n","          self.eval()\n","          # Freeze the encoder and decoder modules\n","          self.encoder.freeze()\n","          self.decoder.freeze()\n","          logging_level = logging.get_verbosity()\n","          logging.set_verbosity(logging.WARNING)\n","          # Work in tmp directory - will store manifest file there\n","          with tempfile.TemporaryDirectory() as tmpdir:\n","              with open(os.path.join(tmpdir, 'manifest.json'), 'w', encoding='utf-8') as fp:\n","                  for audio_file in paths2audio_files:\n","                      entry = {'audio_filepath': audio_file, 'duration': 100000, 'text': ''}\n","                      fp.write(json.dumps(entry) + '\\n')\n","\n","              config = {\n","                  'paths2audio_files': paths2audio_files,\n","                  'batch_size': batch_size,\n","                  'temp_dir': tmpdir,\n","                  'num_workers': num_workers,\n","              }\n","\n","              temporary_datalayer = self._setup_transcribe_dataloader(config)\n","              for test_batch in tqdm(temporary_datalayer, desc=\"Transcribing\"):\n","                  logits, logits_len, greedy_predictions, self_attention_outputs = self.forward(\n","                      input_signal=test_batch[0].to(device), input_signal_length=test_batch[1].to(device)\n","                  )\n","                  if logprobs:\n","                      # dump log probs per file\n","                      # for idx in range(logits.shape[0]):\n","                      #     lg = logits[idx][: logits_len[idx]]\n","                      #     hypotheses.append(lg.cpu().numpy())\n","                      hypotheses = logits\n","                  elif return_self_attention_outputs:\n","                      # dump self attention layer outputs per file\n","                      # for idx in range(logits.shape[0]):\n","                      #     sal = self_attention_outputs[idx][: logits_len[idx]]\n","                      #     hypotheses.append(sal.cpu().numpy())\n","                      hypotheses = self_attention_outputs\n","                  else:\n","                      current_hypotheses = self._wer.ctc_decoder_predictions_tensor(\n","                          greedy_predictions, predictions_len=logits_len, return_hypotheses=return_hypotheses,\n","                      )\n","\n","                      if return_hypotheses:\n","                          # dump log probs per file\n","                          for idx in range(logits.shape[0]):\n","                              current_hypotheses[idx].y_sequence = logits[idx][: logits_len[idx]]\n","\n","                      hypotheses += current_hypotheses\n","\n","                  del greedy_predictions\n","                  del logits\n","                  del self_attention_outputs\n","                  del test_batch\n","      finally:\n","          # set mode back to its original value\n","          self.train(mode=mode)\n","          self.preprocessor.featurizer.dither = dither_value\n","          self.preprocessor.featurizer.pad_to = pad_to_value\n","          if mode is True:\n","              self.encoder.unfreeze()\n","              self.decoder.unfreeze()\n","          logging.set_verbosity(logging_level)\n","      return hypotheses\n","\n","  @property\n","  def input_types(self) -> Optional[Dict[str, NeuralType]]:\n","      if hasattr(self.preprocessor, '_sample_rate'):\n","          input_signal_eltype = AudioSignal(freq=self.preprocessor._sample_rate)\n","      else:\n","          input_signal_eltype = AudioSignal()\n","      return {\n","          \"input_signal\": NeuralType(('B', 'T'), input_signal_eltype, optional=True),\n","          \"input_signal_length\": NeuralType(tuple('B'), LengthsType(), optional=True),\n","          \"processed_signal\": NeuralType(('B', 'D', 'T'), SpectrogramType(), optional=True),\n","          \"processed_signal_length\": NeuralType(tuple('B'), LengthsType(), optional=True),\n","          \"sample_id\": NeuralType(tuple('B'), LengthsType(), optional=True),\n","      }\n","\n","  @property\n","  def output_types(self) -> Optional[Dict[str, NeuralType]]:\n","      return {\n","          \"outputs\": NeuralType(('B', 'T', 'D'), LogprobsType()),\n","          \"encoded_lengths\": NeuralType(tuple('B'), LengthsType()),\n","          \"greedy_predictions\": NeuralType(('B', 'T'), LabelsType()),\n","          \"self_attention_outputs\": NeuralType(('B', 'T', 'D'), LogprobsType()),\n","      }\n","\n","  @typecheck()\n","  def forward(\n","      self, input_signal=None, input_signal_length=None, processed_signal=None, processed_signal_length=None\n","  ):\n","      \"\"\"\n","      Forward pass of the model.\n","\n","      Args:\n","          input_signal: Tensor that represents a batch of raw audio signals,\n","              of shape [B, T]. T here represents timesteps, with 1 second of audio represented as\n","              `self.sample_rate` number of floating point values.\n","          input_signal_length: Vector of length B, that contains the individual lengths of the audio\n","              sequences.\n","          processed_signal: Tensor that represents a batch of processed audio signals,\n","              of shape (B, D, T) that has undergone processing via some DALI preprocessor.\n","          processed_signal_length: Vector of length B, that contains the individual lengths of the\n","              processed audio sequences.\n","\n","      Returns:\n","          A tuple of 4 elements -\n","          1) The log probabilities tensor of shape [B, T, D].\n","          2) The lengths of the acoustic sequence after propagation through the encoder, of shape [B].\n","          3) The greedy token predictions of the model of shape [B, T] (via argmax)\n","          4) The outputs of self attention layer, of shape [B, T, D]\n","      \"\"\"\n","      has_input_signal = input_signal is not None and input_signal_length is not None\n","      has_processed_signal = processed_signal is not None and processed_signal_length is not None\n","      if (has_input_signal ^ has_processed_signal) == False:\n","          raise ValueError(\n","              f\"{self} Arguments ``input_signal`` and ``input_signal_length`` are mutually exclusive \"\n","              \" with ``processed_signal`` and ``processed_signal_len`` arguments.\"\n","          )\n","\n","      if not has_processed_signal:\n","          processed_signal, processed_signal_length = self.preprocessor(\n","              input_signal=input_signal, length=input_signal_length,\n","          )\n","\n","      if self.spec_augmentation is not None and self.training:\n","          processed_signal = self.spec_augmentation(input_spec=processed_signal, length=processed_signal_length)\n","\n","      encoded, encoded_len, self_attention_outputs = self.encoder(audio_signal=processed_signal, length=processed_signal_length)\n","      log_probs = self.decoder(encoder_output=encoded)\n","      greedy_predictions = log_probs.argmax(dim=-1, keepdim=False)\n","\n","      return log_probs, encoded_len, greedy_predictions, self_attention_outputs\n","\n","  def training_step(self, batch, batch_nb):\n","    signal, signal_len, transcript, transcript_len, teacher_logits, teacher_feature_map = batch\n","    if isinstance(batch, DALIOutputs) and batch.has_processed_signal:\n","      log_probs, encoded_len, predictions, self_attention_outputs = self.forward(\n","          processed_signal=signal, processed_signal_length=signal_len\n","      )\n","    else:\n","        log_probs, encoded_len, predictions, self_attention_outputs = self.forward(input_signal=signal, input_signal_length=signal_len)\n","\n","    # Accessing parameters' values\n","    log_probs = log_probs\n","    targets = transcript\n","    input_lengths = encoded_len\n","    target_lengths = transcript_len\n","    student_feature_map = self_attention_outputs\n","\n","    loss_value = self.loss(log_probs, targets, input_lengths, target_lengths, teacher_logits, teacher_feature_map, student_feature_map)\n","    tensorboard_logs = {'train_loss': loss_value, 'learning_rate': self._optimizer.param_groups[0]['lr']}\n","\n","    if hasattr(self, '_trainer') and self._trainer is not None:\n","        log_every_n_steps = self._trainer.log_every_n_steps\n","    else:\n","        log_every_n_steps = 1\n","\n","    if (batch_nb + 1) % log_every_n_steps == 0:\n","        self._wer.update(\n","            predictions=predictions,\n","            targets=transcript,\n","            target_lengths=transcript_len,\n","            predictions_lengths=encoded_len,\n","        )\n","        wer, _, _ = self._wer.compute()\n","        self._wer.reset()\n","        tensorboard_logs.update({'training_batch_wer': wer})\n","\n","    return {'loss': loss_value, 'log': tensorboard_logs}\n","\n","  def predict_step(self, batch, batch_idx, dataloader_idx=0):\n","      signal, signal_len, transcript, transcript_len, teacher_logits, teacher_feature_map, sample_id = batch\n","      if isinstance(batch, DALIOutputs) and batch.has_processed_signal:\n","          log_probs, encoded_len, predictions, self_attention_outputs = self.forward(\n","              processed_signal=signal, processed_signal_length=signal_len\n","          )\n","      else:\n","          log_probs, encoded_len, predictions, self_attention_outputs = self.forward(input_signal=signal, input_signal_length=signal_len)\n","\n","      transcribed_texts = self._wer.ctc_decoder_predictions_tensor(\n","          predictions=predictions, predictions_len=encoded_len, return_hypotheses=False,\n","      )\n","\n","      sample_id = sample_id.cpu().detach().numpy()\n","      return list(zip(sample_id, transcribed_texts))\n","\n","  def validation_step(self, batch, batch_idx, dataloader_idx=0):\n","      signal, signal_len, transcript, transcript_len, teacher_logits, teacher_feature_map = batch\n","      if isinstance(batch, DALIOutputs) and batch.has_processed_signal:\n","          log_probs, encoded_len, predictions, self_attention_outputs = self.forward(\n","              processed_signal=signal, processed_signal_length=signal_len\n","          )\n","      else:\n","          log_probs, encoded_len, predictions, self_attention_outputs = self.forward(input_signal=signal, input_signal_length=signal_len)\n","\n","      self.loss = TotalLoss(\n","        num_classes=self.decoder.num_classes_with_blank - 1,\n","        reduction=self._cfg.get(\"ctc_reduction\", \"mean_batch\"),\n","        # hyper parameters\n","        temperature=3,\n","        gamma=500,\n","        beta=0.03,\n","      )\n","\n","      # Accessing parameters' values\n","      log_probs = log_probs\n","      targets = transcript\n","      input_lengths = encoded_len\n","      target_lengths = transcript_len\n","      student_feature_map = self_attention_outputs\n","\n","      loss_value = self.loss(log_probs, targets, input_lengths, target_lengths, teacher_logits, teacher_feature_map, student_feature_map)\n","      self._wer.update(\n","          predictions=predictions, targets=transcript, target_lengths=transcript_len, predictions_lengths=encoded_len\n","      )\n","      wer, wer_num, wer_denom = self._wer.compute()\n","      self._wer.reset()\n","      return {\n","          'val_loss': loss_value,\n","          'val_wer_num': wer_num,\n","          'val_wer_denom': wer_denom,\n","          'val_wer': wer,\n","      }"],"metadata":{"id":"tGZfrBJz6rSI","executionInfo":{"status":"ok","timestamp":1656158360771,"user_tz":-330,"elapsed":854,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["Creating List of Paths for transcribing"],"metadata":{"id":"T8Zz0pp57SYf"}},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/Paper 1 Implementation/slices/test')\n","directory = 'recordings2'\n","manifest_dir_url = '/content/drive/MyDrive/Colab Notebooks/Paper 1 Implementation/slices/test/recordings2'\n","list_of_paths = []\n","\n","for filename in os.listdir(directory):\n","    list_of_paths.append(f\"{manifest_dir_url}/{filename}\")"],"metadata":{"id":"DNssoxoh7LgQ","executionInfo":{"status":"ok","timestamp":1656158375074,"user_tz":-330,"elapsed":1646,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["teacher_model = ModifiedModel.from_pretrained(\"stt_en_conformer_ctc_large\")"],"metadata":{"id":"0PLPTDkFKAlx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# transcribe() - will show that forward is working too\n","teacher_feature_map = teacher_model.transcribe(paths2audio_files=list_of_paths, batch_size = 4, return_self_attention_outputs = True)\n","print(type(teacher_feature_map))\n","print(teacher_feature_map)\n","teacher_logits = teacher_model.transcribe(paths2audio_files=list_of_paths, batch_size = 4, logprobs = True)\n","print(type(teacher_logits))\n","print(teacher_logits)"],"metadata":{"id":"9qDWXymXyN2r","colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"status":"error","timestamp":1656158429573,"user_tz":-330,"elapsed":596,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}},"outputId":"0fa6bb52-4b91-421c-85f4-c685d2a977f9"},"execution_count":12,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-18e21254f56e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# transcribe() - will show that forward is working too\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mteacher_feature_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths2audio_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_of_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_self_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_feature_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteacher_feature_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mteacher_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths2audio_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist_of_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-ceac024c82de>\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(self, paths2audio_files, batch_size, logprobs, return_hypotheses, return_self_attention_outputs, num_workers)\u001b[0m\n\u001b[1;32m    150\u001b[0m               }\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m               \u001b[0mtemporary_datalayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_transcribe_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m               \u001b[0;32mfor\u001b[0m \u001b[0mtest_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemporary_datalayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Transcribing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                   logits, logits_len, greedy_predictions, self_attention_outputs = self.forward(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nemo/collections/asr/models/ctc_bpe_models.py\u001b[0m in \u001b[0;36m_setup_transcribe_dataloader\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    181\u001b[0m         }\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mtemporary_datalayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_dataloader_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDictConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtemporary_datalayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-ceac024c82de>\u001b[0m in \u001b[0;36m_setup_dataloader_from_config\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         dataset = modifiedDataset.get_bpe_dataset(\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmentor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n","\u001b[0;31mNameError\u001b[0m: name 'modifiedDataset' is not defined"]}]},{"cell_type":"code","source":["teacher_model = EncDecCTCModelBPE.from_pretrained(\"stt_en_conformer_ctc_large\")\n","teacher_logits = teacher_model.transcribe(paths2audio_files=list_of_paths, batch_size = 4, logprobs = True)\n","for i in range(len(teacher_logits)):\n","  print(len(teacher_logits[i]))\n","  for j in range(len(teacher_logits[i])):\n","    print(len(teacher_logits[i][j]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7652e719f43143dca9c61ecdde26b715","8a7e306c54104937afcacf882ac38394","e49d8d7bde7b4f20b897f6e6f53324f0","d2e8321e6b8c4b5d9106e33449ed0b89","ac0ffba13d5d4d6f9d1bdcc9f3aca73e","42caaa978ab44a9791b8a691e3342f09","dbdf438a63d94fd99078b51f3ffcc6c3","3ec2b969903241d3999081c9a6271268","8ea54643cc0d4e65b3fba35a7f3aca02","580ec694d65048afbed5b3b64bfb4cf0","129c57e92d2b4653b894906f9654df85"]},"id":"iVb-wcjqXbOe","executionInfo":{"status":"ok","timestamp":1656136829197,"user_tz":-330,"elapsed":10189,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}},"outputId":"1caf8c18-5bf1-4cda-c068-15d064bd2258"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[NeMo I 2022-06-25 06:00:18 cloud:56] Found existing object /root/.cache/torch/NeMo/NeMo_1.9.0/stt_en_conformer_ctc_large/010120d9959425c7862c9843960b3235/stt_en_conformer_ctc_large.nemo.\n","[NeMo I 2022-06-25 06:00:18 cloud:62] Re-using file from: /root/.cache/torch/NeMo/NeMo_1.9.0/stt_en_conformer_ctc_large/010120d9959425c7862c9843960b3235/stt_en_conformer_ctc_large.nemo\n","[NeMo I 2022-06-25 06:00:18 common:789] Instantiating model from pre-trained checkpoint\n","[NeMo I 2022-06-25 06:00:24 mixins:168] Tokenizer SentencePieceTokenizer initialized with 128 tokens\n"]},{"output_type":"stream","name":"stderr","text":["[NeMo W 2022-06-25 06:00:24 modelPT:149] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n","    Train config : \n","    manifest_filepath: /data/NeMo_ASR_SET/English/v2.0/train/tarred_audio_manifest.json\n","    sample_rate: 16000\n","    batch_size: 32\n","    shuffle: true\n","    num_workers: 8\n","    pin_memory: true\n","    use_start_end_token: false\n","    trim_silence: false\n","    max_duration: 20.0\n","    min_duration: 0.1\n","    shuffle_n: 2048\n","    is_tarred: true\n","    tarred_audio_filepaths: /data/NeMo_ASR_SET/English/v2.0/train/audio__OP_0..4095_CL_.tar\n","    \n","[NeMo W 2022-06-25 06:00:24 modelPT:156] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n","    Validation config : \n","    manifest_filepath:\n","    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n","    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n","    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n","    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n","    sample_rate: 16000\n","    batch_size: 16\n","    shuffle: false\n","    num_workers: 8\n","    pin_memory: true\n","    use_start_end_token: false\n","    is_tarred: false\n","    tarred_audio_filepaths: na\n","    \n","[NeMo W 2022-06-25 06:00:24 modelPT:162] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n","    Test config : \n","    manifest_filepath:\n","    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-other.json\n","    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-clean.json\n","    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-dev-other.json\n","    - /data/ASR/LibriSpeech/librispeech_withsp2/manifests/librivox-test-clean.json\n","    sample_rate: 16000\n","    batch_size: 16\n","    shuffle: false\n","    num_workers: 8\n","    pin_memory: true\n","    use_start_end_token: false\n","    is_tarred: false\n","    tarred_audio_filepaths: na\n","    \n"]},{"output_type":"stream","name":"stdout","text":["[NeMo I 2022-06-25 06:00:24 features:200] PADDING: 0\n"]},{"output_type":"stream","name":"stderr","text":["[NeMo W 2022-06-25 06:00:25 nemo_logging:349] /usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n","                    not been set for this class (WER). The property determines if `update` by\n","                    default needs access to the full metric state. If this is not the case, significant speedups can be\n","                    achieved and we recommend setting this to `False`.\n","                    We provide an checking function\n","                    `from torchmetrics.utilities import check_forward_no_full_state`\n","                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n","                    default for now) or if `full_state_update=False` can be used safely.\n","                    \n","      warnings.warn(*args, **kwargs)\n","    \n","[NeMo W 2022-06-25 06:00:25 nemo_logging:349] /usr/local/lib/python3.7/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n","                    not been set for this class (WERBPE). The property determines if `update` by\n","                    default needs access to the full metric state. If this is not the case, significant speedups can be\n","                    achieved and we recommend setting this to `False`.\n","                    We provide an checking function\n","                    `from torchmetrics.utilities import check_forward_no_full_state`\n","                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n","                    default for now) or if `full_state_update=False` can be used safely.\n","                    \n","      warnings.warn(*args, **kwargs)\n","    \n"]},{"output_type":"stream","name":"stdout","text":["[NeMo I 2022-06-25 06:00:26 save_restore_connector:243] Model EncDecCTCModelBPE was successfully restored from /root/.cache/torch/NeMo/NeMo_1.9.0/stt_en_conformer_ctc_large/010120d9959425c7862c9843960b3235/stt_en_conformer_ctc_large.nemo.\n"]},{"output_type":"display_data","data":{"text/plain":["Transcribing:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7652e719f43143dca9c61ecdde26b715"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[NeMo W 2022-06-25 06:00:28 nemo_logging:349] /usr/local/lib/python3.7/dist-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","      warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n","    \n"]},{"output_type":"stream","name":"stdout","text":["10\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","13\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","12\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n","129\n"]}]}]}