{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"overwrittenEncoder.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMF6co329dVcOtTsBEsZL8o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GdwaUjntwGrL","executionInfo":{"status":"ok","timestamp":1655877528564,"user_tz":-330,"elapsed":3407,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}},"outputId":"2cee12b9-10bb-46bb-8aab-1bc624b58eb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/Colab Notebooks/Paper 1 Implementation\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63mkC-j8wGic","executionInfo":{"status":"ok","timestamp":1655877528565,"user_tz":-330,"elapsed":8,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}},"outputId":"a2697831-b4e2-441e-aed9-8671fb41c5f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Paper 1 Implementation\n"]}]},{"cell_type":"code","source":["!pip install nemo_toolkit['all']\n","!pip install hydra-core==1.1"],"metadata":{"id":"D3WEbGw2wGX9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install import-ipynb"],"metadata":{"id":"zOn8AcvrwOZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","from collections import OrderedDict\n","from typing import List, Optional\n","\n","import torch\n","import torch.distributed\n","import torch.nn as nn\n","import import_ipynb\n","import os"],"metadata":{"id":"MlUswuKEwQB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from omegaconf import DictConfig, OmegaConf, open_dict\n","from nemo.collections.asr.modules.conformer_encoder import ConformerEncoder\n","from nemo.collections.asr.parts.submodules.multi_head_attention import PositionalEncoding, RelPositionalEncoding\n","from nemo.collections.asr.parts.submodules.subsampling import ConvSubsampling, StackingSubsampling\n","from nemo.core.classes.common import typecheck\n","from nemo.core.classes.mixins import adapter_mixins\n","from nemo.core.classes.exportable import Exportable\n","from nemo.core.classes.module import NeuralModule\n","from nemo.core.neural_types import AcousticEncodedRepresentation, LengthsType, NeuralType, SpectrogramType\n","%run overwrittenConformerLayer.ipynb import OverwrittenConformerLayer"],"metadata":{"id":"Lcl-qnWDwSLq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["__all__ = ['OverwrittenEncoder']"],"metadata":{"id":"39Wygoa6wYPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g54HdDPCvzB0"},"outputs":[],"source":["class OverwrittenEncoder(ConformerEncoder, NeuralModule, Exportable):\n","\n","  @property\n","  def input_types(self):\n","      \"\"\"Returns definitions of module input ports.\"\"\"\n","      return OrderedDict(\n","          {\n","              \"audio_signal\": NeuralType(('B', 'D', 'T'), SpectrogramType()),\n","              \"length\": NeuralType(tuple('B'), LengthsType()),\n","          }\n","      )\n","\n","  @property\n","  def output_types(self):\n","      \"\"\"Returns definitions of module output ports.\"\"\"\n","      return OrderedDict(\n","          {\n","              \"outputs\": NeuralType(('B', 'D', 'T'), AcousticEncodedRepresentation()),\n","              \"encoded_lengths\": NeuralType(tuple('B'), LengthsType()),\n","              \"self_attention_outputs\": NeuralType(('B', 'T', 'D'), AcousticEncodedRepresentation())\n","          }\n","      )\n","\n","  def __init__(self, feat_in, n_layers, d_model, feat_out=-1, subsampling='striding', subsampling_factor=4, subsampling_conv_channels=-1,\n","               ff_expansion_factor=4, self_attention_model='rel_pos', n_heads=8, att_context_size=None, xscaling=True, untie_biases=True, \n","               pos_emb_max_len=5000, conv_kernel_size=31, conv_norm_type='batch_norm', dropout=0.1, dropout_emb=0.1, dropout_att=0.1):\n","      super().__init__(\n","          feat_in=feat_in,\n","          n_layers=n_layers,\n","          d_model=d_model,\n","          feat_out=-1,\n","          subsampling='striding',\n","          subsampling_factor=4,\n","          subsampling_conv_channels=-1,\n","          ff_expansion_factor=4,\n","          self_attention_model='rel_pos',\n","          n_heads=8,\n","          att_context_size=None,\n","          xscaling=True,\n","          untie_biases=True,\n","          pos_emb_max_len=5000,\n","          conv_kernel_size=31,\n","          conv_norm_type='batch_norm',\n","          dropout=0.1,\n","          dropout_emb=0.1,\n","          dropout_att=0.1,\n","      )\n","\n","      d_ff = d_model * ff_expansion_factor\n","      self.d_model = d_model\n","      self._feat_in = feat_in\n","      self.scale = math.sqrt(self.d_model)\n","      if att_context_size:\n","          self.att_context_size = att_context_size\n","      else:\n","          self.att_context_size = [-1, -1]\n","\n","      if xscaling:\n","          self.xscale = math.sqrt(d_model)\n","      else:\n","          self.xscale = None\n","\n","      if subsampling_conv_channels == -1:\n","          subsampling_conv_channels = d_model\n","      if subsampling and subsampling_factor > 1:\n","          if subsampling == 'stacking':\n","              self.pre_encode = StackingSubsampling(\n","                  subsampling_factor=subsampling_factor, feat_in=feat_in, feat_out=d_model\n","              )\n","          else:\n","              self.pre_encode = ConvSubsampling(\n","                  subsampling=subsampling,\n","                  subsampling_factor=subsampling_factor,\n","                  feat_in=feat_in,\n","                  feat_out=d_model,\n","                  conv_channels=subsampling_conv_channels,\n","                  activation=nn.ReLU(),\n","              )\n","      else:\n","          self.pre_encode = nn.Linear(feat_in, d_model)\n","\n","      self._feat_out = d_model\n","\n","      if not untie_biases and self_attention_model == \"rel_pos\":\n","          d_head = d_model // n_heads\n","          pos_bias_u = nn.Parameter(torch.Tensor(n_heads, d_head))\n","          pos_bias_v = nn.Parameter(torch.Tensor(n_heads, d_head))\n","          nn.init.zeros_(pos_bias_u)\n","          nn.init.zeros_(pos_bias_v)\n","      else:\n","          pos_bias_u = None\n","          pos_bias_v = None\n","\n","      self.pos_emb_max_len = pos_emb_max_len\n","      if self_attention_model == \"rel_pos\":\n","          self.pos_enc = RelPositionalEncoding(\n","              d_model=d_model,\n","              dropout_rate=dropout,\n","              max_len=pos_emb_max_len,\n","              xscale=self.xscale,\n","              dropout_rate_emb=dropout_emb,\n","          )\n","      elif self_attention_model == \"abs_pos\":\n","          pos_bias_u = None\n","          pos_bias_v = None\n","          self.pos_enc = PositionalEncoding(\n","              d_model=d_model, dropout_rate=dropout, max_len=pos_emb_max_len, xscale=self.xscale\n","          )\n","      else:\n","          raise ValueError(f\"Not valid self_attention_model: '{self_attention_model}'!\")\n","\n","      self.layers = nn.ModuleList()\n","      for i in range(n_layers):\n","          layer = OverwrittenConformerLayer(\n","              d_model=d_model,\n","              d_ff=d_ff,\n","              self_attention_model=self_attention_model,\n","              n_heads=n_heads,\n","              conv_kernel_size=conv_kernel_size,\n","              conv_norm_type=conv_norm_type,\n","              dropout=dropout,\n","              dropout_att=dropout_att,\n","              pos_bias_u=pos_bias_u,\n","              pos_bias_v=pos_bias_v,\n","          )\n","          self.layers.append(layer)\n","\n","      if feat_out > 0 and feat_out != self._feat_out:\n","          self.out_proj = nn.Linear(self._feat_out, feat_out)\n","          self._feat_out = feat_out\n","      else:\n","          self.out_proj = None\n","          self._feat_out = d_model\n","      self.set_max_audio_length(self.pos_emb_max_len)\n","      self.use_pad_mask = True\n","\n","  @typecheck()\n","  def forward_for_export(self, audio_signal, length):\n","      max_audio_length: int = audio_signal.size(-1)\n","\n","      if max_audio_length > self.max_audio_length:\n","          self.set_max_audio_length(max_audio_length)\n","\n","      if length is None:\n","          length = audio_signal.new_full(\n","              audio_signal.size(0), max_audio_length, dtype=torch.int32, device=self.seq_range.device\n","          )\n","\n","      audio_signal = torch.transpose(audio_signal, 1, 2)\n","\n","      if isinstance(self.pre_encode, nn.Linear):\n","          audio_signal = self.pre_encode(audio_signal)\n","      else:\n","          audio_signal, length = self.pre_encode(audio_signal, length)\n","\n","      audio_signal, pos_emb = self.pos_enc(audio_signal)\n","      # adjust size\n","      max_audio_length = audio_signal.size(1)\n","      # Create the self-attention and padding masks\n","\n","      pad_mask = self.make_pad_mask(max_audio_length, length)\n","      att_mask = pad_mask.unsqueeze(1).repeat([1, max_audio_length, 1])\n","      att_mask = torch.logical_and(att_mask, att_mask.transpose(1, 2))\n","      if self.att_context_size[0] >= 0:\n","          att_mask = att_mask.triu(diagonal=-self.att_context_size[0])\n","      if self.att_context_size[1] >= 0:\n","          att_mask = att_mask.tril(diagonal=self.att_context_size[1])\n","      att_mask = ~att_mask\n","\n","      if self.use_pad_mask:\n","          pad_mask = ~pad_mask\n","      else:\n","          pad_mask = None\n","\n","      for lth, layer in enumerate(self.layers):\n","          audio_signal, self_attention_outputs = layer(x=audio_signal, att_mask=att_mask, pos_emb=pos_emb, pad_mask=pad_mask)\n","\n","      if self.out_proj is not None:\n","          audio_signal = self.out_proj(audio_signal)\n","\n","      audio_signal = torch.transpose(audio_signal, 1, 2)\n","      return audio_signal, length, self_attention_outputs"]},{"cell_type":"code","source":["# audio_signal = torch.randn((8, 80, 8))\n","# length = torch.randn((8))\n","# encoder = OverwrittenEncoder(feat_in=80, n_layers=9, d_model=8)\n","# print(encoder(audio_signal=audio_signal, length=length)[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FIxSdBGyDS02","executionInfo":{"status":"ok","timestamp":1655886336410,"user_tz":-330,"elapsed":697,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}},"outputId":"bade7de3-dc66-4f7d-9fae-e93dd54b6868"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619],\n","         [-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619]],\n","\n","        [[-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619],\n","         [-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619]],\n","\n","        [[-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619],\n","         [-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619]],\n","\n","        [[-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619],\n","         [-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619]],\n","\n","        [[-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619],\n","         [-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619]],\n","\n","        [[ 0.2878,  0.7090, -0.3247, -0.4488, -0.0891,  0.2139, -0.1136,\n","           0.3657],\n","         [-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619]],\n","\n","        [[ 0.0221,  0.5545, -0.6648, -0.8504, -0.3224, -0.2894,  0.0080,\n","           0.1548],\n","         [-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619]],\n","\n","        [[-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619],\n","         [-0.0732,  0.3078, -0.3505, -0.2861, -0.1897,  0.3093,  0.1758,\n","           0.2619]]], grad_fn=<AddBackward0>)\n"]}]}]}