{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"modified_audio_to_text_dali.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOToSkPjkhKDMZzf4fqIUrA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zVsF7buMplyZ"},"outputs":[],"source":["!pip install nemo_toolkit['all']\n","!pip install hydra-core==1.1"]},{"cell_type":"code","source":["import math\n","import operator\n","import os.path\n","import os\n","import io\n","import time\n","from collections.abc import Iterator\n","from typing import Callable, List, Optional, Union\n","\n","import torch\n","import pickle\n","from omegaconf import DictConfig\n","\n","from nemo.collections.asr.data.audio_to_text import ASRManifestProcessor, expand_audio_filepaths\n","from nemo.collections.common.parts.preprocessing import parsers\n","from nemo.utils import logging, model_utils\n","\n","try:\n","    import nvidia.dali as dali\n","    from nvidia.dali.pipeline import Pipeline\n","    from nvidia.dali.plugin.pytorch import DALIGenericIterator as DALIPytorchIterator\n","    from nvidia.dali.plugin.pytorch import LastBatchPolicy as LastBatchPolicy\n","\n","    HAVE_DALI = True\n","except (ImportError, ModuleNotFoundError):\n","    HAVE_DALI = False"],"metadata":{"id":"iK8qmloSSV1x","executionInfo":{"status":"ok","timestamp":1656151113985,"user_tz":-330,"elapsed":3,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["__all__ = [\n","    'AudioToCharDALIDataset',\n","    'AudioToBPEDALIDataset',\n","]"],"metadata":{"id":"dFonX2zVTInO","executionInfo":{"status":"ok","timestamp":1656151120481,"user_tz":-330,"elapsed":385,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["__DALI_MINIMUM_VERSION__ = \"1.11\"\n","\n","DALI_INSTALLATION_MESSAGE = (\n","    \"Could not import `nvidia.dali`.\\n\"\n","    \"Please install DALI by following the steps provided here - \\n\"\n","    \"https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html\"\n",")"],"metadata":{"id":"MWbZhyqFTKEu","executionInfo":{"status":"ok","timestamp":1656151145413,"user_tz":-330,"elapsed":366,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def is_dali_supported(min_version: str, verbose: bool = False) -> bool:\n","    \"\"\"\n","    Checks if DALI in installed, and version is >= min_verion.\n","\n","    Args:\n","        min_version: A semver str that is the minimum requirement.\n","        verbose: Whether to log the installation instructions if DALI is not found.\n","\n","    Returns:\n","        bool - whether DALI could be imported or not.\n","    \"\"\"\n","    module_available, _ = model_utils.check_lib_version(\n","        'nvidia.dali', checked_version=min_version, operator=operator.ge\n","    )\n","\n","    # If DALI is not installed\n","    if module_available is None:\n","        if verbose:\n","            logging.info(DALI_INSTALLATION_MESSAGE)\n","\n","        return False\n","\n","    return module_available"],"metadata":{"id":"Mkt5Qq8BTP0q","executionInfo":{"status":"ok","timestamp":1656151165236,"user_tz":-330,"elapsed":382,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class DALIOutputs(object):\n","    def __init__(self, out_dict):\n","        self._has_processed_signal = 'processed_signal' in out_dict and 'processed_signal_len' in out_dict\n","        if not self._has_processed_signal:\n","            assert 'audio' in out_dict and 'audio_len' in out_dict\n","        assert 'transcript' in out_dict and 'transcript_len' in out_dict\n","        assert 'logits' in out_dict and 'maps' in out_dict\n","\n","        if self._has_processed_signal:\n","            self._outs = (\n","                out_dict['processed_signal'],\n","                out_dict['processed_signal_len'].reshape(-1),\n","                out_dict['transcript'],\n","                out_dict['transcript_len'].reshape(-1),\n","                out_dict['logits'],\n","                out_dict['maps'],\n","            )\n","        else:\n","            self._outs = (\n","                out_dict['audio'],\n","                out_dict['audio_len'].reshape(-1),\n","                out_dict['transcript'],\n","                out_dict['transcript_len'].reshape(-1),\n","                out_dict['logits'],\n","                out_dict['maps'],\n","            )\n","\n","    @property\n","    def has_processed_signal(self):\n","        return self._has_processed_signal\n","\n","    def __getitem__(self, key):\n","        return self._outs[key]\n","\n","    def __len__(self):\n","        return len(self._outs)"],"metadata":{"id":"HGDIGyMJTVFI","executionInfo":{"status":"ok","timestamp":1656151799494,"user_tz":-330,"elapsed":357,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class _AudioTextDALIDataset(Iterator):\n","    \"\"\"\n","    NVIDIA DALI pipeline that loads tensors via one or more manifest files where each line containing a sample descriptor in JSON,\n","    including audio files, transcripts, and durations (in seconds).\n","    Here's an example:\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text_filepath\": \"/path/to/audio.txt\", \"duration\": 23.147}\n","    ...\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"the transcription\", \"offset\": 301.75, \"duration\": 0.82, \"utt\":\n","    \"utterance_id\", \"ctm_utt\": \"en_4156\", \"side\": \"A\"}\n","\n","    Args:\n","        manifest_filepath: Path to manifest file with the format described above. Can be comma-separated paths.\n","        device (str): Determines the device type to be used for preprocessing. Allowed values are: 'cpu', 'gpu'.\n","        batch_size (int): Number of samples in a batch.\n","        parser (str, callable): A str for an inbuilt parser, or a callable with signature f(str) -> List[int].\n","        sample_rate (int): Sample rate to resample loaded audio to.\n","        num_threads (int): Number of CPU processing threads to be created by the DALI pipeline.\n","        max_duration (float): Determines the maximum allowed duration, in seconds, of the loaded audio files.\n","        min_duration (float): Determines the minimum allowed duration, in seconds, of the loaded audio files.\n","        bos_id (int): Id of beginning of sequence symbol to append if not None\n","        eos_id (int): Id of end of sequence symbol to append if not None\n","        pad_id (int): Id used to pad the input. Defaults to 0 if not provided.\n","        trim (bool): If True, it will extract the nonsilent region of the loaded audio signal.\n","        shuffle (bool): If set to True, the dataset will shuffled after loading.\n","        drop_last (bool): If set to True, the last batch will be dropped if incomplete. This will be the case when the shard size is not divisible by the batch size.\n","                          If set to False and the size of dataset is not divisible by the batch size, then the last batch will be smaller.\n","        device_id (int): Index of the GPU to be used (local_rank). Only applicable when device == 'gpu'. Defaults to 0.\n","        global_rank (int): Worker rank, used for partitioning shards. Defaults to 0.\n","        world_size (int): Total number of processes, used for partitioning shards. Defaults to 1.\n","        preprocessor_cfg (DictConfig): Preprocessor configuration. Supports AudioToMelSpectrogramPreprocessor and AudioToMFCCPreprocessor.\n","        return_sample_id (bool): whether to return the sample_id as a part of each sample (not supported yet).\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        manifest_filepath: str,\n","        device: str,\n","        batch_size: int,\n","        parser: Union[str, Callable],\n","        audio_tar_filepaths: Optional[Union[str, List[str]]] = None,\n","        audio_tar_index_filepaths: Optional[Union[str, List[str]]] = None,\n","        sample_rate: int = 16000,\n","        num_threads: int = 4,\n","        max_duration: float = 0.0,\n","        min_duration: float = 0.0,\n","        bos_id: Optional[int] = None,\n","        eos_id: Optional[int] = None,\n","        pad_id: int = 0,\n","        trim: bool = False,\n","        shuffle: bool = False,\n","        drop_last: bool = False,\n","        shard_strategy: str = \"scatter\",\n","        device_id: int = 0,\n","        global_rank: int = 0,\n","        world_size: int = 1,\n","        preprocessor_cfg: DictConfig = None,\n","        return_sample_id: bool = False,\n","    ):\n","        self.drop_last = drop_last  # used by lr_scheduler\n","        if return_sample_id:\n","            raise ValueError(\n","                \"Currently DALI data layers don't support returning the sample_id and return_sample_id can not be enabled.\"\n","            )\n","        self.return_sample_id = return_sample_id\n","\n","        if not HAVE_DALI:\n","            raise ModuleNotFoundError(\n","                f\"{self} requires NVIDIA DALI to be installed. \"\n","                f\"See: https://docs.nvidia.com/deeplearning/dali/user-guide/docs/installation.html#id1\"\n","            )\n","\n","        if device not in ('cpu', 'gpu'):\n","            raise ValueError(\n","                f\"{self} received an unexpected device argument {device}. Supported values are: 'cpu', 'gpu'\"\n","            )\n","\n","        device_id = device_id if device == 'gpu' else None\n","\n","        self.batch_size = batch_size  # Used by NeMo\n","\n","        self.device = device\n","        self.device_id = device_id\n","\n","        if world_size > 1:\n","            self.shard_id = global_rank\n","            self.num_shards = world_size\n","        else:\n","            self.shard_id = None\n","            self.num_shards = None\n","\n","        self.eos_id = eos_id\n","        self.bos_id = bos_id\n","        self.sample_rate = sample_rate\n","\n","        self.pipe = Pipeline(\n","            batch_size=batch_size,\n","            num_threads=num_threads,\n","            device_id=self.device_id,\n","            exec_async=True,\n","            exec_pipelined=True,\n","        )\n","\n","        has_preprocessor = preprocessor_cfg is not None\n","        if has_preprocessor:\n","            if preprocessor_cfg._target_ == \"nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor\":\n","                feature_type = \"mel_spectrogram\"\n","            elif preprocessor_cfg._target_ == \"nemo.collections.asr.modules.AudioToMFCCPreprocessor\":\n","                feature_type = \"mfcc\"\n","            else:\n","                raise ValueError(\n","                    f\"{self} received an unexpected preprocessor configuration: {preprocessor_cfg._target_}.\"\n","                    f\" Supported preprocessors are: AudioToMelSpectrogramPreprocessor, AudioToMFCCPreprocessor\"\n","                )\n","\n","            # Default values taken from AudioToMelSpectrogramPreprocessor\n","            params = preprocessor_cfg\n","            self.dither = params['dither'] if 'dither' in params else 0.0\n","            self.preemph = params['preemph'] if 'preemph' in params else 0.97\n","            self.window_size_sec = params['window_size'] if 'window_size' in params else 0.02\n","            self.window_stride_sec = params['window_stride'] if 'window_stride' in params else 0.01\n","            self.sample_rate = params['sample_rate'] if 'sample_rate' in params else sample_rate\n","            self.window_size = int(self.window_size_sec * self.sample_rate)\n","            self.window_stride = int(self.window_stride_sec * self.sample_rate)\n","\n","            normalize = params['normalize'] if 'normalize' in params else 'per_feature'\n","            if normalize == 'per_feature':  # Each freq channel independently\n","                self.normalization_axes = (1,)\n","            elif normalize == 'all_features':\n","                self.normalization_axes = (0, 1)\n","            else:\n","                raise ValueError(\n","                    f\"{self} received {normalize} for the normalize parameter.\"\n","                    f\" It must be either 'per_feature' or 'all_features'.\"\n","                )\n","\n","            self.window = None\n","            window_name = params['window'] if 'window' in params else 'hann'\n","            torch_windows = {\n","                'hann': torch.hann_window,\n","                'hamming': torch.hamming_window,\n","                'blackman': torch.blackman_window,\n","                'bartlett': torch.bartlett_window,\n","                'none': None,\n","            }\n","\n","            if window_name == 'ones':\n","                window_tensor = torch.ones(self.window_size)\n","            else:\n","                try:\n","                    window_fn = torch_windows.get(window_name, None)\n","                except:\n","                    raise ValueError(\n","                        f\"{self} received '{window_name}' for the window parameter.\"\n","                        f\" It must be one of: ('hann', 'ones', 'hamming', 'blackman', 'bartlett', None).\"\n","                        f\" None is equivalent to 'hann'.\"\n","                    )\n","                window_tensor = window_fn(self.window_size, periodic=False) if window_fn else None\n","            self.window = window_tensor.numpy().tolist() if window_tensor is not None else None\n","\n","            self.n_fft = params['n_fft'] if 'n_fft' in params else 2 ** math.ceil(math.log2(self.window_size))\n","            self.n_mels = params['n_mels'] if 'n_mels' in params else 64\n","            self.n_mfcc = params['n_mfcc'] if 'n_mfcc' in params else 64\n","\n","            features = params['features'] if 'features' in params else 0\n","            if features > 0:\n","                if feature_type == 'mel_spectrogram':\n","                    self.n_mels = features\n","                elif feature_type == 'mfcc':\n","                    self.n_mfcc = features\n","\n","            # TODO Implement frame splicing\n","            if 'frame_splicing' in params:\n","                assert params['frame_splicing'] == 1, \"Frame splicing is not implemented\"\n","\n","            self.freq_low = params['lowfreq'] if 'lowfreq' in params else 0.0\n","            self.freq_high = params['highfreq'] if 'highfreq' in params else self.sample_rate / 2.0\n","            self.log_features = params['log'] if 'log' in params else True\n","\n","            # We want to avoid taking the log of zero\n","            # There are two options: either adding or clamping to a small value\n","\n","            self.log_zero_guard_type = params['log_zero_guard_type'] if 'log_zero_guard_type' in params else 'add'\n","            if self.log_zero_guard_type not in [\"add\", \"clamp\"]:\n","                raise ValueError(\n","                    f\"{self} received {self.log_zero_guard_type} for the \"\n","                    f\"log_zero_guard_type parameter. It must be either 'add' or \"\n","                    f\"'clamp'.\"\n","                )\n","\n","            self.log_zero_guard_value = (\n","                params['log_zero_guard_value'] if 'log_zero_guard_value' in params else 2 ** -24\n","            )\n","            if isinstance(self.log_zero_guard_value, str):\n","                if self.log_zero_guard_value == \"tiny\":\n","                    self.log_zero_guard_value = torch.finfo(torch.float32).tiny\n","                elif self.log_zero_guard_value == \"eps\":\n","                    self.log_zero_guard_value = torch.finfo(torch.float32).eps\n","                else:\n","                    raise ValueError(\n","                        f\"{self} received {self.log_zero_guard_value} for the log_zero_guard_type parameter.\"\n","                        f\"It must be either a number, 'tiny', or 'eps'\"\n","                    )\n","\n","            self.mag_power = params['mag_power'] if 'mag_power' in params else 2\n","            if self.mag_power != 1.0 and self.mag_power != 2.0:\n","                raise ValueError(\n","                    f\"{self} received {self.mag_power} for the mag_power parameter.\" f\" It must be either 1.0 or 2.0.\"\n","                )\n","\n","            self.pad_to = max(params['pad_to'], 1) if 'pad_to' in params else 16\n","            self.pad_value = params['pad_value'] if 'pad_value' in params else 0.0\n","\n","        with self.pipe:\n","            if audio_tar_filepaths is None and audio_tar_index_filepaths is None:\n","                audio, indices = dali.fn.readers.nemo_asr(\n","                    name=\"Reader\",\n","                    manifest_filepaths=manifest_filepath.split(','),\n","                    dtype=dali.types.FLOAT,\n","                    downmix=True,\n","                    sample_rate=float(self.sample_rate),\n","                    min_duration=min_duration,\n","                    max_duration=max_duration,\n","                    read_sample_rate=False,\n","                    read_text=False,\n","                    read_idxs=True,\n","                    random_shuffle=shuffle,\n","                    shard_id=self.shard_id,\n","                    num_shards=self.num_shards,\n","                    pad_last_batch=True,\n","                )\n","\n","                self.is_tarred_dataset = False\n","\n","            elif audio_tar_filepaths is not None and audio_tar_index_filepaths is not None:\n","                audio_tar_filepaths = expand_audio_filepaths(\n","                    audio_tar_filepaths, shard_strategy=shard_strategy, world_size=world_size, global_rank=global_rank\n","                )\n","                audio_tar_index_filepaths = expand_audio_filepaths(\n","                    audio_tar_index_filepaths,\n","                    shard_strategy=shard_strategy,\n","                    world_size=world_size,\n","                    global_rank=global_rank,\n","                )\n","\n","                if len(audio_tar_filepaths) != len(audio_tar_index_filepaths) and len(audio_tar_index_filepaths) != 0:\n","                    raise ValueError(\n","                        f\"Number of filepaths provided for `audio_tar_filepaths` must match \"\n","                        f\"`audio_tar_index_filepaths`. Got {len(audio_tar_filepaths)} audio_tar_filepaths and \"\n","                        f\"{len(audio_tar_index_filepaths)} audio_tar_index_filepaths.\"\n","                    )\n","\n","                tar_file = dali.fn.readers.webdataset(\n","                    paths=audio_tar_filepaths,\n","                    index_paths=audio_tar_index_filepaths,\n","                    name=\"Reader\",\n","                    ext=[\"wav\"],\n","                    missing_component_behavior=\"error\",\n","                    random_shuffle=shuffle,\n","                    shard_id=self.shard_id,\n","                    num_shards=self.num_shards,\n","                    pad_last_batch=True,\n","                )\n","                audio, _ = dali.fn.decoders.audio(\n","                    tar_file, dtype=dali.types.FLOAT, downmix=True, sample_rate=float(self.sample_rate),\n","                )\n","                indices = dali.fn.get_property(tar_file, key=\"source_info\")\n","                indices = dali.fn.pad(indices)\n","\n","                self.is_tarred_dataset = True\n","\n","            else:\n","                raise RuntimeError(\n","                    \"When using DALI datasets, either `audio_tar_filepaths` \"\n","                    \"and `audio_tar_index_filepaths` should either both be None (sequential dataset)\"\n","                    \"or provided (tarred dataset).\"\n","                )\n","\n","            # Extract nonsilent region, if necessary\n","            if trim:\n","                # Need to extract non-silent region before moving to the GPU\n","                roi_start, roi_len = dali.fn.nonsilent_region(audio, cutoff_db=-60)\n","                audio = audio.gpu() if self.device == 'gpu' else audio\n","                audio = dali.fn.slice(\n","                    audio, roi_start, roi_len, normalized_anchor=False, normalized_shape=False, axes=[0]\n","                )\n","            else:\n","                audio = audio.gpu() if self.device == 'gpu' else audio\n","\n","            if not has_preprocessor:\n","                # No preprocessing, the output is the audio signal\n","                audio_len = dali.fn.shapes(dali.fn.reshape(audio, shape=[-1]))\n","                audio = dali.fn.pad(audio)\n","                self.pipe.set_outputs(audio, audio_len, indices)\n","            else:\n","                # Additive gaussian noise (dither)\n","                if self.dither > 0.0:\n","                    gaussian_noise = dali.fn.random.normal(audio)\n","                    audio = audio + self.dither * gaussian_noise\n","\n","                # Preemphasis filter\n","                if self.preemph > 0.0:\n","                    audio = dali.fn.preemphasis_filter(audio, preemph_coeff=self.preemph, border='zero')\n","\n","                # Power spectrogram\n","                spec = dali.fn.spectrogram(\n","                    audio,\n","                    nfft=self.n_fft,\n","                    window_length=self.window_size,\n","                    window_step=self.window_stride,\n","                    window_fn=self.window,\n","                )\n","\n","                if feature_type == 'mel_spectrogram' or feature_type == 'mfcc':\n","                    # Spectrogram to Mel Spectrogram\n","                    spec = dali.fn.mel_filter_bank(\n","                        spec,\n","                        sample_rate=self.sample_rate,\n","                        nfilter=self.n_mels,\n","                        normalize=True,\n","                        freq_low=self.freq_low,\n","                        freq_high=self.freq_high,\n","                    )\n","                    # Mel Spectrogram to MFCC\n","                    if feature_type == 'mfcc':\n","                        spec = dali.fn.mfcc(spec, n_mfcc=self.n_mfcc)\n","\n","                # Logarithm\n","                if self.log_zero_guard_type == 'add':\n","                    spec = spec + self.log_zero_guard_value\n","\n","                spec = dali.fn.to_decibels(\n","                    spec, multiplier=math.log(10), reference=1.0, cutoff_db=math.log(self.log_zero_guard_value)\n","                )\n","\n","                # Normalization\n","                spec = dali.fn.normalize(spec, axes=self.normalization_axes, epsilon=1e-5 ** 2, ddof=1)\n","\n","                # Extracting the length of the spectrogram\n","                spec_len = dali.fn.slice(dali.fn.shapes(spec), 1, 1, axes=(0,))\n","\n","                # Pads feature dimension to be a multiple of `pad_to` and the temporal dimension to be as big as the largest sample (shape -1)\n","                spec = dali.fn.pad(spec, fill_value=self.pad_value, axes=(0, 1), align=(self.pad_to, 1), shape=(1, -1))\n","                self.pipe.set_outputs(spec, spec_len, indices)\n","\n","        x = time.time()\n","        # Building DALI pipeline\n","        self.pipe.build()\n","        y = time.time()\n","\n","        logging.info(f\"Time for pipe.build() : {(y - x)} seconds\")\n","\n","        if has_preprocessor:\n","            output_names = ['processed_signal', 'processed_signal_len', 'manifest_indices']\n","        else:\n","            output_names = ['audio', 'audio_len', 'manifest_indices']\n","\n","        x = time.time()\n","        last_batch_policy = LastBatchPolicy.DROP if drop_last else LastBatchPolicy.PARTIAL\n","        self._iter = DALIPytorchIterator(\n","            [self.pipe],\n","            output_map=output_names,\n","            reader_name=\"Reader\",\n","            last_batch_policy=last_batch_policy,\n","            dynamic_shape=True,\n","            auto_reset=True,\n","        )\n","        y = time.time()\n","        logging.info(f\"Time for DALIPytorchIterator to initialize : {(y - x)} seconds\")\n","\n","        # TODO come up with a better solution\n","        class DummyDataset:\n","            def __init__(self, parent):\n","                self.parent = parent\n","\n","            def __len__(self):\n","                return self.parent.size\n","\n","        self.dataset = DummyDataset(self)  # Used by NeMo\n","\n","        x = time.time()\n","        self.manifest_processor = ASRManifestProcessor(\n","            manifest_filepath=manifest_filepath,\n","            parser=parser,\n","            max_duration=max_duration,\n","            min_duration=min_duration,\n","            max_utts=0,\n","            bos_id=bos_id,\n","            eos_id=eos_id,\n","            pad_id=pad_id,\n","            index_by_file_id=self.is_tarred_dataset,\n","        )\n","        y = time.time()\n","        logging.info(f\"Time to build nemo manifest processor - {(y - x)} seconds\")\n","\n","    def reset(self):\n","        self._iter.reset()\n","\n","    def __iter__(self):\n","        return self\n","\n","    def next(self):\n","        return self.__next__()\n","\n","    @property\n","    def size(self):\n","        return self._iter.size\n","\n","    def __len__(self):\n","        return len(self._iter)\n","\n","    def __next__(self):\n","        outputs = self._iter.next()\n","        assert len(outputs) == 1\n","        dali_out = outputs[0]\n","        manifest_indices = dali_out['manifest_indices'].numpy()\n","\n","        out = {}\n","        out_names = ['processed_signal', 'processed_signal_len', 'audio', 'audio_len']\n","        for out_name in out_names:\n","            if out_name in dali_out:\n","                out[out_name] = dali_out[out_name].detach().clone()\n","\n","        text_tokens = []\n","        text_tokens_len = []\n","        max_len = 0\n","        batch_size = manifest_indices.shape[0]\n","        for i, manifest_index in enumerate(manifest_indices):\n","\n","            if not self.is_tarred_dataset:\n","                # Loose-file dataset. Index is integer based.\n","                manifest_index = manifest_index[0]\n","                text, text_length = self.manifest_processor.process_text_by_id(manifest_index)\n","            else:\n","                # Tarred-file dataset. Index is filename based.\n","                resolved_manifest_indices = manifest_index.tobytes().decode().split(\":\")\n","                resolved_manifest_index = resolved_manifest_indices[2]  # we require just the filename segment\n","                resolved_manifest_index = os.path.splitext(resolved_manifest_index)[0]  # we dont need file extension\n","                text, text_length = self.manifest_processor.process_text_by_file_id(resolved_manifest_index)\n","\n","            text_tokens_len.append(text_length)\n","            text_tokens.append(text)\n","            if text_length > max_len:\n","                max_len = text_length\n","\n","        transcript_out = torch.full([batch_size, max_len], fill_value=self.manifest_processor.pad_id, dtype=torch.long)\n","        for i, n in enumerate(text_tokens_len):\n","            transcript_out[i, :n] = torch.tensor(text_tokens[i], dtype=torch.long)\n","        transcript_len_out = torch.tensor(text_tokens_len, dtype=torch.long)\n","\n","        # Loading objects (data) from files (on Disk) to restore their values to memory\n","        # Loading teacher_logits\n","        file = \"teacher_logits.pkl\"\n","        file_obj = open(file, \"rb\") \n","        teacher_logits = pickle.load(file_obj) # Tensor\n","\n","        # Loading teacher_feature_map\n","        file = \"teacher_feature_map.pkl\"\n","        file_obj = open(file, \"rb\") # read binary\n","        teacher_feature_map = pickle.load(file_obj) # Tensor\n","\n","        out['transcript'] = transcript_out\n","        out['transcript_len'] = transcript_len_out\n","        out['logits'] = teacher_logits\n","        out['maps'] = teacher_feature_map\n","\n","        return DALIOutputs(out)"],"metadata":{"id":"rge1D-v8VwGl","executionInfo":{"status":"ok","timestamp":1656153058351,"user_tz":-330,"elapsed":1830,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class AudioToCharDALIDataset(_AudioTextDALIDataset):\n","    \"\"\"\n","    Character based NVIDIA DALI pipeline that loads tensors via one or more manifest files where each line containing a\n","    sample descriptor in JSON, including audio files, transcripts, and durations (in seconds).\n","    Here's an example:\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text_filepath\": \"/path/to/audio.txt\", \"duration\": 23.147}\n","    ...\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"the transcription\", \"offset\": 301.75, \"duration\": 0.82, \"utt\":\n","    \"utterance_id\", \"ctm_utt\": \"en_4156\", \"side\": \"A\"}\n","\n","    Args:\n","        manifest_filepath: Path to manifest file with the format described above. Can be comma-separated paths.\n","        device (str): Determines the device type to be used for preprocessing. Allowed values are: 'cpu', 'gpu'.\n","        batch_size (int): Number of samples in a batch.\n","        labels (List[str]): String containing all the possible characters to map to.\n","        sample_rate (int): Sample rate to resample loaded audio to.\n","        num_threads (int): Number of CPU processing threads to be created by the DALI pipeline.\n","        max_duration (float): Determines the maximum allowed duration, in seconds, of the loaded audio files.\n","        min_duration (float): Determines the minimum allowed duration, in seconds, of the loaded audio files.\n","        blank_index (int): blank character index, default = -1\n","        unk_index (int): unk_character index, default = -1\n","        normalize (bool): whether to normalize transcript text (default): True\n","        bos_id (int): Id of beginning of sequence symbol to append if not None\n","        eos_id (int): Id of end of sequence symbol to append if not None\n","        pad_id (int): Id used to pad the input. Defaults to 0 if not provided.\n","        trim (bool): If True, it will extract the nonsilent region of the loaded audio signal.\n","        shuffle (bool): If set to True, the dataset will shuffled after loading.\n","        drop_last (bool): If set to True, the last batch will be dropped if incomplete. This will be the case when the shard size is not divisible by the batch size.\n","                          If set to False and the size of dataset is not divisible by the batch size, then the last batch will be smaller.\n","        parser (str, callable): A str for an inbuilt parser, or a callable with signature f(str) -> List[int].\n","        device_id (int): Index of the GPU to be used (local_rank). Only applicable when device == 'gpu'. Defaults to 0.\n","        global_rank (int): Worker rank, used for partitioning shards. Defaults to 0.\n","        world_size (int): Total number of processes, used for partitioning shards. Defaults to 1.\n","        preprocessor_cfg (DictConfig): Preprocessor configuration. Supports AudioToMelSpectrogramPreprocessor and AudioToMFCCPreprocessor.\n","        return_sample_id (bool): whether to return the sample_id as a part of each sample (not supported yet).\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        manifest_filepath: str,\n","        device: str,\n","        batch_size: int,\n","        labels: Union[str, List[str]],\n","        sample_rate: int = 16000,\n","        audio_tar_filepaths: Optional[Union[str, List[str]]] = None,\n","        audio_tar_index_filepaths: Optional[Union[str, List[str]]] = None,\n","        num_threads: int = 4,\n","        max_duration: float = 0.0,\n","        min_duration: float = 0.0,\n","        blank_index: int = -1,\n","        unk_index: int = -1,\n","        normalize: bool = True,\n","        bos_id: Optional[int] = None,\n","        eos_id: Optional[int] = None,\n","        pad_id: int = 0,\n","        trim: bool = False,\n","        shuffle: bool = False,\n","        drop_last: bool = False,\n","        parser: Union[str, Callable] = 'en',\n","        shard_strategy: str = \"scatter\",\n","        device_id: int = 0,\n","        global_rank: int = 0,\n","        world_size: int = 1,\n","        preprocessor_cfg: DictConfig = None,\n","        return_sample_id: bool = False,\n","    ):\n","        self.labels = labels\n","\n","        parser = parsers.make_parser(\n","            labels=labels, name=parser, unk_id=unk_index, blank_id=blank_index, do_normalize=normalize\n","        )\n","\n","        super().__init__(\n","            manifest_filepath=manifest_filepath,\n","            device=device,\n","            batch_size=batch_size,\n","            audio_tar_filepaths=audio_tar_filepaths,\n","            audio_tar_index_filepaths=audio_tar_index_filepaths,\n","            sample_rate=sample_rate,\n","            num_threads=num_threads,\n","            max_duration=max_duration,\n","            min_duration=min_duration,\n","            bos_id=bos_id,\n","            eos_id=eos_id,\n","            pad_id=pad_id,\n","            trim=trim,\n","            shuffle=shuffle,\n","            drop_last=drop_last,\n","            parser=parser,\n","            shard_strategy=shard_strategy,\n","            device_id=device_id,\n","            global_rank=global_rank,\n","            world_size=world_size,\n","            preprocessor_cfg=preprocessor_cfg,\n","            return_sample_id=return_sample_id,\n","        )\n"],"metadata":{"id":"sfeb_5SKaliW","executionInfo":{"status":"ok","timestamp":1656153129896,"user_tz":-330,"elapsed":358,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class AudioToBPEDALIDataset(_AudioTextDALIDataset):\n","    \"\"\"\n","    Subword based NVIDIA DALI pipeline that loads tensors via one or more manifest files where each line containing a\n","    sample descriptor in JSON, including audio files, transcripts, and durations (in seconds).\n","    Here's an example:\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text_filepath\": \"/path/to/audio.txt\", \"duration\": 23.147}\n","    ...\n","    {\"audio_filepath\": \"/path/to/audio.wav\", \"text\": \"the transcription\", \"offset\": 301.75, \"duration\": 0.82, \"utt\":\n","    \"utterance_id\", \"ctm_utt\": \"en_4156\", \"side\": \"A\"}\n","\n","    Args:\n","        manifest_filepath: Path to manifest file with the format described above. Can be comma-separated paths.\n","        tokenizer (TokenizerSpec): A TokenizerSpec implementation that wraps a tokenization implementation.\n","        device (str): Determines the device type to be used for preprocessing. Allowed values are: 'cpu', 'gpu'.\n","        batch_size (int): Number of samples in a batch.\n","        sample_rate (int): Sample rate to resample loaded audio to.\n","        num_threads (int): Number of CPU processing threads to be created by the DALI pipeline.\n","        max_duration (float): Determines the maximum allowed duration, in seconds, of the loaded audio files.\n","        min_duration (float): Determines the minimum allowed duration, in seconds, of the loaded audio files.\n","        bos_id (int): Id of beginning of sequence symbol to append if not None. Injected from the tokenizer.\n","        eos_id (int): Id of end of sequence symbol to append if not None. Injected from the tokenizer.\n","        pad_id (int): Id used to pad the input. Defaults to 0 if not provided. Injected from the tokenizer.\n","        trim (bool): If True, it will extract the nonsilent region of the loaded audio signal.\n","        shuffle (bool): If set to True, the dataset will shuffled after loading.\n","        drop_last (bool): If set to True, the last batch will be dropped if incomplete. This will be the case when the shard size is not divisible by the batch size.\n","                          If set to False and the size of dataset is not divisible by the batch size, then the last batch will be smaller.\n","\n","        device_id (int): Index of the GPU to be used (local_rank). Only applicable when device == 'gpu'. Defaults to 0.\n","        global_rank (int): Worker rank, used for partitioning shards. Defaults to 0.\n","        world_size (int): Total number of processes, used for partitioning shards. Defaults to 1.\n","        preprocessor_cfg (DictConfig): Preprocessor configuration. Supports AudioToMelSpectrogramPreprocessor and AudioToMFCCPreprocessor.\n","        use_start_end_token (bool): Boolean which dictates whether to add [BOS] and [EOS] tokens to beginning and\n","            ending of speech respectively.\n","        return_sample_id (bool): whether to return the sample_id as a part of each sample (not supported yet).\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        manifest_filepath: str,\n","        tokenizer: 'nemo.collections.common.tokenizers.TokenizerSpec',\n","        device: str,\n","        batch_size: int,\n","        sample_rate: int = 16000,\n","        audio_tar_filepaths: Optional[Union[str, List[str]]] = None,\n","        audio_tar_index_filepaths: Optional[Union[str, List[str]]] = None,\n","        num_threads: int = 4,\n","        max_duration: float = 0.0,\n","        min_duration: float = 0.0,\n","        trim: bool = False,\n","        shuffle: bool = False,\n","        drop_last: bool = False,\n","        shard_strategy: str = \"scatter\",\n","        device_id: int = 0,\n","        global_rank: int = 0,\n","        world_size: int = 1,\n","        preprocessor_cfg: DictConfig = None,\n","        use_start_end_token: bool = True,\n","        return_sample_id: bool = False,\n","    ):\n","\n","        if use_start_end_token and hasattr(tokenizer, 'bos_token'):\n","            bos_id = tokenizer.bos_id\n","        else:\n","            bos_id = None\n","\n","        if use_start_end_token and hasattr(tokenizer, 'eos_token'):\n","            eos_id = tokenizer.eos_id\n","        else:\n","            eos_id = None\n","\n","        if hasattr(tokenizer, 'pad_token'):\n","            pad_id = tokenizer.pad_id\n","        else:\n","            pad_id = 0\n","\n","        class TokenizerWrapper:\n","            def __init__(self, tokenizer):\n","                self._tokenizer = tokenizer\n","\n","            def __call__(self, text):\n","                t = self._tokenizer.text_to_ids(text)\n","                return t\n","\n","        super().__init__(\n","            manifest_filepath=manifest_filepath,\n","            device=device,\n","            batch_size=batch_size,\n","            sample_rate=sample_rate,\n","            audio_tar_filepaths=audio_tar_filepaths,\n","            audio_tar_index_filepaths=audio_tar_index_filepaths,\n","            num_threads=num_threads,\n","            max_duration=max_duration,\n","            min_duration=min_duration,\n","            bos_id=bos_id,\n","            eos_id=eos_id,\n","            pad_id=pad_id,\n","            trim=trim,\n","            shuffle=shuffle,\n","            drop_last=drop_last,\n","            parser=TokenizerWrapper(tokenizer),\n","            shard_strategy=shard_strategy,\n","            device_id=device_id,\n","            global_rank=global_rank,\n","            world_size=world_size,\n","            preprocessor_cfg=preprocessor_cfg,\n","            return_sample_id=return_sample_id,\n","        )\n"],"metadata":{"id":"te5hQ_Sza016","executionInfo":{"status":"ok","timestamp":1656153163707,"user_tz":-330,"elapsed":412,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":11,"outputs":[]}]}