{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"modifiedDataset.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMCitfc2rJVTvCYUXmbt5Ly"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"23OLp1awtz_D"},"outputs":[],"source":["!pip install nemo_toolkit['all']\n","!pip install hydra-core==1.1"]},{"cell_type":"code","source":["import json\n","import random\n","from typing import Any, List, Optional, Union\n","\n","import torch\n","from omegaconf import DictConfig, open_dict\n","from omegaconf.listconfig import ListConfig\n","from pytorch_lightning.callbacks import BasePredictionWriter\n","from torch.utils.data import ChainDataset\n","\n","import modified_audio_to_text, modified_audio_to_text_dali\n","from nemo.utils import logging"],"metadata":{"id":"-r1njBcneYk6","executionInfo":{"status":"ok","timestamp":1656070951490,"user_tz":-330,"elapsed":2,"user":{"displayName":"Apoorva Aggarwal","userId":"06292853917120984205"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def get_bpe_dataset(\n","    config: dict, tokenizer: 'TokenizerSpec', augmentor: Optional['AudioAugmentor'] = None\n",") -> modified_audio_to_text.AudioToBPEDataset:\n","    \"\"\"\n","    Instantiates a Byte Pair Encoding / Word Piece Encoding based AudioToBPEDataset.\n","\n","    Args:\n","        config: Config of the AudioToBPEDataset.\n","        tokenizer: An instance of a TokenizerSpec object.\n","        augmentor: Optional AudioAugmentor object for augmentations on audio data.\n","\n","    Returns:\n","        An instance of AudioToBPEDataset.\n","    \"\"\"\n","    dataset = modified_audio_to_text.AudioToBPEDataset(\n","        manifest_filepath=config['manifest_filepath'],\n","        tokenizer=tokenizer,\n","        sample_rate=config['sample_rate'],\n","        int_values=config.get('int_values', False),\n","        augmentor=augmentor,\n","        max_duration=config.get('max_duration', None),\n","        min_duration=config.get('min_duration', None),\n","        max_utts=config.get('max_utts', 0),\n","        trim=config.get('trim_silence', False),\n","        use_start_end_token=config.get('use_start_end_token', True),\n","        return_sample_id=config.get('return_sample_id', False),\n","    )\n","    return dataset"],"metadata":{"id":"MYgwWoAJe0HB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_dali_bpe_dataset(\n","    config: dict,\n","    tokenizer,\n","    shuffle: bool,\n","    device_id: int,\n","    global_rank: int,\n","    world_size: int,\n","    preprocessor_cfg: Optional[DictConfig] = None,\n",") -> modified_audio_to_text_dali.AudioToCharDALIDataset:\n","    \"\"\"\n","    Instantiates a Subword Encoding based AudioToBPEDALIDataset.\n","\n","    Args:\n","        config: Config of the AudioToBPEDALIDataset.\n","        tokenizer: An implementation of NeMo TokenizerSpec.\n","        shuffle: Bool flag whether to shuffle the dataset.\n","        device_id: Index of the GPU to be used (local_rank). Only applicable when device == 'gpu'. Defaults to 0.\n","        global_rank: Global rank of this device.\n","        world_size: Global world size in the training method.\n","        preprocessor_cfg: Preprocessor configuration. Supports AudioToMelSpectrogramPreprocessor and AudioToMFCCPreprocessor.\n","\n","    Returns:\n","        An instance of AudioToCharDALIDataset.\n","    \"\"\"\n","    device = 'gpu' if torch.cuda.is_available() else 'cpu'\n","    dataset = modified_audio_to_text_dali.AudioToBPEDALIDataset(\n","        manifest_filepath=config['manifest_filepath'],\n","        tokenizer=tokenizer,\n","        device=device,\n","        batch_size=config['batch_size'],\n","        sample_rate=config['sample_rate'],\n","        audio_tar_filepaths=config.get('tarred_audio_filepaths', None),\n","        audio_tar_index_filepaths=config.get('tarred_audio_index_filepaths', None),\n","        max_duration=config.get('max_duration', None),\n","        min_duration=config.get('min_duration', None),\n","        trim=config.get('trim_silence', False),\n","        use_start_end_token=config.get('use_start_end_token', True),\n","        shuffle=shuffle,\n","        shard_strategy=config.get('tarred_shard_strategy', 'scatter'),\n","        device_id=device_id,\n","        global_rank=global_rank,\n","        world_size=world_size,\n","        preprocessor_cfg=preprocessor_cfg,\n","        return_sample_id=config.get('return_sample_id', False),\n","    )\n","    return dataset"],"metadata":{"id":"moFvDjvoiejD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_tarred_dataset(\n","    config: dict,\n","    shuffle_n: int,\n","    global_rank: int,\n","    world_size: int,\n","    tokenizer: Optional['TokenizerSpec'] = None,\n","    augmentor: Optional['AudioAugmentor'] = None,\n",") -> Union[modified_audio_to_text.TarredAudioToBPEDataset, modified_audio_to_text.TarredAudioToCharDataset]:\n","    \"\"\"\n","    Instantiates a Word Piece/BPE Encoding based TarredAudioToBPEDataset or a char based TarredAudioToCharDataset.\n","\n","    Args:\n","        config: Config of the TarredAudioToBPEDataset or TarredAudioToCharDataset.\n","        shuffle_n: How many samples to look ahead and load to be shuffled.\n","            See WebDataset documentation for more details.\n","        tokenizer: An instance of a TokenizerSpec object if BPE dataset is needed.\n","        global_rank: Global rank of this device.\n","        world_size: Global world size in the training method.\n","            Passsing None would return a char-based dataset.\n","        augmentor: Optional AudioAugmentor object for augmentations on audio data.\n","\n","    Returns:\n","        An instance of TarredAudioToBPEDataset or TarredAudioToCharDataset.\n","    \"\"\"\n","    tarred_audio_filepaths = config['tarred_audio_filepaths']\n","    manifest_filepaths = config['manifest_filepath']\n","    datasets = []\n","    tarred_audio_filepaths = convert_to_config_list(tarred_audio_filepaths)\n","    manifest_filepaths = convert_to_config_list(manifest_filepaths)\n","\n","    if len(manifest_filepaths) != len(tarred_audio_filepaths):\n","        raise ValueError(\n","            f\"manifest_filepaths (length={len(manifest_filepaths)}) and tarred_audio_filepaths (length={len(tarred_audio_filepaths)}) need to have the same number of buckets.\"\n","        )\n","\n","    if 'labels' not in config:\n","        logging.warning(f\"dataset does not have explicitly defined labels\")\n","\n","    for dataset_idx, (tarred_audio_filepath, manifest_filepath) in enumerate(\n","        zip(tarred_audio_filepaths, manifest_filepaths)\n","    ):\n","        if len(tarred_audio_filepath) == 1:\n","            tarred_audio_filepath = tarred_audio_filepath[0]\n","        if tokenizer is None:\n","            dataset = modified_audio_to_text.TarredAudioToCharDataset(\n","                audio_tar_filepaths=tarred_audio_filepath,\n","                manifest_filepath=manifest_filepath,\n","                labels=config.get('labels', None),\n","                sample_rate=config['sample_rate'],\n","                int_values=config.get('int_values', False),\n","                augmentor=augmentor,\n","                shuffle_n=shuffle_n,\n","                max_duration=config.get('max_duration', None),\n","                min_duration=config.get('min_duration', None),\n","                max_utts=config.get('max_utts', 0),\n","                blank_index=config.get('blank_index', -1),\n","                unk_index=config.get('unk_index', -1),\n","                normalize=config.get('normalize_transcripts', False),\n","                trim=config.get('trim_silence', False),\n","                parser=config.get('parser', 'en'),\n","                shard_strategy=config.get('tarred_shard_strategy', 'scatter'),\n","                global_rank=global_rank,\n","                world_size=world_size,\n","                return_sample_id=config.get('return_sample_id', False),\n","            )\n","        else:\n","            dataset = modified_audio_to_text.TarredAudioToBPEDataset(\n","                audio_tar_filepaths=tarred_audio_filepath,\n","                manifest_filepath=manifest_filepath,\n","                tokenizer=tokenizer,\n","                sample_rate=config['sample_rate'],\n","                int_values=config.get('int_values', False),\n","                augmentor=augmentor,\n","                shuffle_n=shuffle_n,\n","                max_duration=config.get('max_duration', None),\n","                min_duration=config.get('min_duration', None),\n","                max_utts=config.get('max_utts', 0),\n","                trim=config.get('trim_silence', False),\n","                use_start_end_token=config.get('use_start_end_token', True),\n","                shard_strategy=config.get('tarred_shard_strategy', 'scatter'),\n","                global_rank=global_rank,\n","                world_size=world_size,\n","                return_sample_id=config.get('return_sample_id', False),\n","            )\n","\n","        datasets.append(dataset)\n","\n","    return get_chain_dataset(datasets=datasets, ds_config=config)"],"metadata":{"id":"T12Sp5TOijmy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_to_config_list(initial_list):\n","    if type(initial_list) is str:\n","        initial_list = initial_list.split(\",\")\n","    if initial_list is None or initial_list == []:\n","        raise ValueError(\"manifest_filepaths and tarred_audio_filepaths must not be empty.\")\n","    if not isinstance(initial_list, ListConfig):\n","        initial_list = ListConfig([initial_list])\n","\n","    for list_idx, list_val in enumerate(initial_list):\n","        if type(list_val) != type(initial_list[0]):\n","            raise ValueError(\n","                \"manifest_filepaths and tarred_audio_filepaths need to be a list of lists for bucketing or just a list of strings\"\n","            )\n","    if type(initial_list[0]) is not ListConfig:\n","        initial_list = ListConfig([initial_list])\n","    return initial_list\n","\n","\n","def get_chain_dataset(datasets, ds_config):\n","    if len(datasets) > 1:\n","        if ds_config.get('bucketing_batch_size', None) is not None:\n","            bucketing_batch_sizes = calc_bucketing_batch_sizes(ds_config, len(datasets))\n","            logging.info(\n","                f\"Batch bucketing is enabled for {len(datasets)} buckets with adaptive batch sizes of {bucketing_batch_sizes}!\"\n","            )\n","            for idx, dataset in enumerate(datasets):\n","                datasets[idx] = modified_audio_to_text.BucketingDataset(\n","                    dataset=dataset, bucketing_batch_size=bucketing_batch_sizes[idx]\n","                )\n","        else:\n","            logging.info(\n","                f\"Batch bucketing is enabled for {len(datasets)} buckets with fixed batch size of {ds_config['batch_size']}!\"\n","            )\n","\n","    if len(datasets) == 1:\n","        return datasets[0]\n","    bucketing_strategy = ds_config.get('bucketing_strategy', 'synced_randomized')\n","    if bucketing_strategy == 'fixed_order':\n","        return ChainDataset(datasets)\n","    elif bucketing_strategy == 'synced_randomized':\n","        return modified_audio_to_text.RandomizedChainDataset(datasets=datasets, rnd_seed=0)\n","    elif bucketing_strategy == 'fully_randomized':\n","        return modified_audio_to_text.RandomizedChainDataset(datasets=datasets, rnd_seed=random.randint(0, 30000))\n","    else:\n","        raise ValueError(\n","            f'bucketing_strategy={bucketing_strategy} is not supported! Supported strategies are [fixed_order, fully_randomized, synced_randomized].'\n","        )\n","\n","\n","def calc_bucketing_batch_sizes(ds_config, datasets_len):\n","    bucketing_batch_size = ds_config['bucketing_batch_size']\n","    if ds_config['batch_size'] != 1:\n","        raise ValueError(\n","            f\"batch_size should be set to one when bucketing_batch_size is set and adaptive bucketing is enabled (batch_size={ds_config['batch_size']}!\"\n","        )\n","    if type(bucketing_batch_size) == int:\n","        bucketing_batch_sizes = []\n","        for idx in range(datasets_len):\n","            scale_factor = datasets_len - idx\n","            bucketing_batch_sizes.append(scale_factor * bucketing_batch_size)\n","    elif isinstance(bucketing_batch_size, ListConfig) or isinstance(bucketing_batch_size, list):\n","        bucketing_batch_sizes = bucketing_batch_size\n","    else:\n","        raise ValueError(\n","            f\"bucketing_batch_size should be an integer or a list (bucketing_batch_size={bucketing_batch_size})!\"\n","        )\n","\n","    if len(bucketing_batch_sizes) != datasets_len:\n","        raise ValueError(\n","            f\"batch_size should have the same length as the number of buckets ({len(bucketing_batch_sizes)}!={datasets_len}) \"\n","        )\n","    return bucketing_batch_sizes"],"metadata":{"id":"ToBm7tI_iqVx"},"execution_count":null,"outputs":[]}]}